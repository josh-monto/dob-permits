[2025-04-14T01:14:35.450+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-04-14T01:14:35.461+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: data_ingestion_gcs_dag.create_temp_table_task scheduled__2025-04-13T00:00:00+00:00 [queued]>
[2025-04-14T01:14:35.466+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: data_ingestion_gcs_dag.create_temp_table_task scheduled__2025-04-13T00:00:00+00:00 [queued]>
[2025-04-14T01:14:35.467+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 2
[2025-04-14T01:14:35.473+0000] {taskinstance.py:2890} INFO - Executing <Task(BigQueryInsertJobOperator): create_temp_table_task> on 2025-04-13 00:00:00+00:00
[2025-04-14T01:14:35.477+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=116) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2025-04-14T01:14:35.478+0000] {standard_task_runner.py:72} INFO - Started process 118 to run task
[2025-04-14T01:14:35.480+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'data_ingestion_gcs_dag', 'create_temp_table_task', 'scheduled__2025-04-13T00:00:00+00:00', '--job-id', '7', '--raw', '--subdir', 'DAGS_FOLDER/data_ingestion_gcs_dag.py', '--cfg-path', '/tmp/tmpemczhky9']
[2025-04-14T01:14:35.481+0000] {standard_task_runner.py:105} INFO - Job 7: Subtask create_temp_table_task
[2025-04-14T01:14:35.507+0000] {task_command.py:467} INFO - Running <TaskInstance: data_ingestion_gcs_dag.create_temp_table_task scheduled__2025-04-13T00:00:00+00:00 [running]> on host 974ab08d9047
[2025-04-14T01:14:35.547+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='data_ingestion_gcs_dag' AIRFLOW_CTX_TASK_ID='create_temp_table_task' AIRFLOW_CTX_EXECUTION_DATE='2025-04-13T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-04-13T00:00:00+00:00'
[2025-04-14T01:14:35.548+0000] {logging_mixin.py:190} INFO - Task instance is in running state
[2025-04-14T01:14:35.548+0000] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2025-04-14T01:14:35.549+0000] {logging_mixin.py:190} INFO - Current task name:create_temp_table_task state:running start_date:2025-04-14 01:14:35.461299+00:00
[2025-04-14T01:14:35.549+0000] {logging_mixin.py:190} INFO - Dag name:data_ingestion_gcs_dag and current dag run status:running
[2025-04-14T01:14:35.549+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-04-14T01:14:35.550+0000] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2025-04-14T01:14:35.550+0000] {connection.py:277} WARNING - Connection schemes (type: google_cloud_platform) shall not contain '_' according to RFC3986.
[2025-04-14T01:14:35.550+0000] {base.py:84} INFO - Retrieving connection 'google_cloud_default'
[2025-04-14T01:14:35.578+0000] {bigquery.py:2665} INFO - Executing: {'query': {'query': "\n          CREATE OR REPLACE TABLE `nyc-projects-455321.building_permits.external_table`\n          AS\n          SELECT\n            borough STRING OPTIONS (description = '1 = Manhattan, 2 = Bronx, 3 = Brooklyn, 4 = Queens, 5 = Staten Island'),\n            job_type STRING OPTIONS (description = 'Job Type, based on DOB Job Code (NB-New Building, A1, A2, A3- Alterations 1-3, SG-Sign, etc.)'),\n            block STRING OPTIONS (description = 'Tax block assigned by Department of Finance'),\n            lot STRING OPTIONS (description = 'Tax lot assigned by Department of Finance'),\n            issuance_date TIMESTAMP OPTIONS (description = 'Issuance Date'),\n            permit_si_no INT OPTIONS (description = 'unique ID for permit'),\n            gis_latitude FLOAT OPTIONS (description = 'Latitude'),\n            gis_longitude FLOAT OPTIONS (description = 'Longitude'),\n            gis_nta_name STRING OPTIONS (description = 'Neighborhood name')\n          FROM `nyc-projects-455321.building_permits.external_table`;\n        ", 'useLegacySql': False}}'
[2025-04-14T01:14:35.579+0000] {bigquery.py:1241} INFO - Inserting job ***_data_ingestion_gcs_dag_create_temp_table_task_2025_04_13T00_00_00_00_00_b8a50db576acab6a458914662c2aab57
[2025-04-14T01:14:35.986+0000] {taskinstance.py:3313} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 768, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 734, in _execute_callable
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 424, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 2731, in execute
    job.result(timeout=self.result_timeout, retry=self.result_retry)
  File "/home/airflow/.local/lib/python3.12/site-packages/google/cloud/bigquery/job/query.py", line 1590, in result
    do_get_result()
  File "/home/airflow/.local/lib/python3.12/site-packages/google/cloud/bigquery/job/query.py", line 1579, in do_get_result
    super(QueryJob, self).result(retry=retry, timeout=timeout)
  File "/home/airflow/.local/lib/python3.12/site-packages/google/cloud/bigquery/job/base.py", line 971, in result
    return super(_AsyncJob, self).result(timeout=timeout, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/google/api_core/future/polling.py", line 261, in result
    raise self._exception
google.api_core.exceptions.BadRequest: 400 Syntax error: Expected end of input but got keyword OPTIONS at [5:28]; reason: invalidQuery, location: query, message: Syntax error: Expected end of input but got keyword OPTIONS at [5:28]

Location: US
Job ID: airflow_data_ingestion_gcs_dag_create_temp_table_task_2025_04_13T00_00_00_00_00_b8a50db576acab6a458914662c2aab57

[2025-04-14T01:14:36.005+0000] {logging_mixin.py:190} INFO - Task instance in failure state
[2025-04-14T01:14:36.005+0000] {logging_mixin.py:190} INFO - Task start:2025-04-14 01:14:35.461299+00:00 end:2025-04-14 01:14:36.004172+00:00 duration:0.542873
[2025-04-14T01:14:36.006+0000] {logging_mixin.py:190} INFO - Task:<Task(BigQueryInsertJobOperator): create_temp_table_task> dag:<DAG: data_ingestion_gcs_dag> dagrun:<DagRun data_ingestion_gcs_dag @ 2025-04-13 00:00:00+00:00: scheduled__2025-04-13T00:00:00+00:00, state:running, queued_at: 2025-04-14 01:14:18.207523+00:00. externally triggered: False>
[2025-04-14T01:14:36.007+0000] {logging_mixin.py:190} INFO - Failure caused by 400 Syntax error: Expected end of input but got keyword OPTIONS at [5:28]; reason: invalidQuery, location: query, message: Syntax error: Expected end of input but got keyword OPTIONS at [5:28]

Location: US
Job ID: ***_data_ingestion_gcs_dag_create_temp_table_task_2025_04_13T00_00_00_00_00_b8a50db576acab6a458914662c2aab57
[2025-04-14T01:14:36.008+0000] {taskinstance.py:1226} INFO - Marking task as UP_FOR_RETRY. dag_id=data_ingestion_gcs_dag, task_id=create_temp_table_task, run_id=scheduled__2025-04-13T00:00:00+00:00, execution_date=20250413T000000, start_date=20250414T011435, end_date=20250414T011436
[2025-04-14T01:14:36.039+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-04-14T01:14:36.039+0000] {standard_task_runner.py:124} ERROR - Failed to execute job 7 for task create_temp_table_task (400 Syntax error: Expected end of input but got keyword OPTIONS at [5:28]; reason: invalidQuery, location: query, message: Syntax error: Expected end of input but got keyword OPTIONS at [5:28]

Location: US
Job ID: airflow_data_ingestion_gcs_dag_create_temp_table_task_2025_04_13T00_00_00_00_00_b8a50db576acab6a458914662c2aab57
; 118)
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py", line 117, in _start_by_fork
    ret = args.func(args, dag=self.dag)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/cli.py", line 116, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 483, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 256, in _run_task_by_selected_method
    return _run_raw_task(args, ti)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 341, in _run_raw_task
    return ti._run_raw_task(
           ^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3006, in _run_raw_task
    return _run_raw_task(
           ^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 274, in _run_raw_task
    TaskInstance._execute_task_with_callbacks(
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3161, in _execute_task_with_callbacks
    result = self._execute_task(context, task_orig)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3185, in _execute_task
    return _execute_task(self, context, task_orig)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 768, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 734, in _execute_callable
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 424, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 2731, in execute
    job.result(timeout=self.result_timeout, retry=self.result_retry)
  File "/home/airflow/.local/lib/python3.12/site-packages/google/cloud/bigquery/job/query.py", line 1590, in result
    do_get_result()
  File "/home/airflow/.local/lib/python3.12/site-packages/google/cloud/bigquery/job/query.py", line 1579, in do_get_result
    super(QueryJob, self).result(retry=retry, timeout=timeout)
  File "/home/airflow/.local/lib/python3.12/site-packages/google/cloud/bigquery/job/base.py", line 971, in result
    return super(_AsyncJob, self).result(timeout=timeout, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/google/api_core/future/polling.py", line 261, in result
    raise self._exception
google.api_core.exceptions.BadRequest: 400 Syntax error: Expected end of input but got keyword OPTIONS at [5:28]; reason: invalidQuery, location: query, message: Syntax error: Expected end of input but got keyword OPTIONS at [5:28]

Location: US
Job ID: airflow_data_ingestion_gcs_dag_create_temp_table_task_2025_04_13T00_00_00_00_00_b8a50db576acab6a458914662c2aab57

[2025-04-14T01:14:36.068+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 1
[2025-04-14T01:14:36.085+0000] {taskinstance.py:3901} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-04-14T01:14:36.086+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-04-14T01:26:20.407+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-04-14T01:26:20.420+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: data_ingestion_gcs_dag.create_temp_table_task scheduled__2025-04-13T00:00:00+00:00 [queued]>
[2025-04-14T01:26:20.425+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: data_ingestion_gcs_dag.create_temp_table_task scheduled__2025-04-13T00:00:00+00:00 [queued]>
[2025-04-14T01:26:20.426+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 2
[2025-04-14T01:26:20.432+0000] {taskinstance.py:2890} INFO - Executing <Task(BigQueryInsertJobOperator): create_temp_table_task> on 2025-04-13 00:00:00+00:00
[2025-04-14T01:26:20.437+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'data_ingestion_gcs_dag', 'create_temp_table_task', 'scheduled__2025-04-13T00:00:00+00:00', '--job-id', '7', '--raw', '--subdir', 'DAGS_FOLDER/data_ingestion_gcs_dag.py', '--cfg-path', '/tmp/tmptbzijp7i']
[2025-04-14T01:26:20.438+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=187) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2025-04-14T01:26:20.438+0000] {standard_task_runner.py:105} INFO - Job 7: Subtask create_temp_table_task
[2025-04-14T01:26:20.439+0000] {standard_task_runner.py:72} INFO - Started process 189 to run task
[2025-04-14T01:26:20.462+0000] {task_command.py:467} INFO - Running <TaskInstance: data_ingestion_gcs_dag.create_temp_table_task scheduled__2025-04-13T00:00:00+00:00 [running]> on host 8681a31054bd
[2025-04-14T01:26:20.504+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='data_ingestion_gcs_dag' AIRFLOW_CTX_TASK_ID='create_temp_table_task' AIRFLOW_CTX_EXECUTION_DATE='2025-04-13T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-04-13T00:00:00+00:00'
[2025-04-14T01:26:20.505+0000] {logging_mixin.py:190} INFO - Task instance is in running state
[2025-04-14T01:26:20.505+0000] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2025-04-14T01:26:20.506+0000] {logging_mixin.py:190} INFO - Current task name:create_temp_table_task state:running start_date:2025-04-14 01:26:20.420405+00:00
[2025-04-14T01:26:20.506+0000] {logging_mixin.py:190} INFO - Dag name:data_ingestion_gcs_dag and current dag run status:running
[2025-04-14T01:26:20.506+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-04-14T01:26:20.506+0000] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2025-04-14T01:26:20.507+0000] {connection.py:277} WARNING - Connection schemes (type: google_cloud_platform) shall not contain '_' according to RFC3986.
[2025-04-14T01:26:20.507+0000] {base.py:84} INFO - Retrieving connection 'google_cloud_default'
[2025-04-14T01:26:20.535+0000] {bigquery.py:2665} INFO - Executing: {'query': {'query': "\n          CREATE OR REPLACE TABLE `nyc-projects-455321.building_permits.external_table`\n          AS\n          SELECT\n            borough,\n            job_type,\n            block,\n            lot,\n            PARSE_TIMESTAMP('%m-%d-%Y', issuance_date) AS issuance_date,\n            permit_si_no INT OPTIONS (description = 'unique ID for permit'),\n            CAST(permit_si_no AS INT64) AS permit_si_no,\n            CAST(gis_latitude AS FLOAT64) AS gis_latitude,\n            CAST(gis_longitude AS FLOAT64) AS gis_longitude,\n            gis_nta_name\n          FROM `nyc-projects-455321.building_permits.external_table`;\n        ", 'useLegacySql': False}}'
[2025-04-14T01:26:20.536+0000] {bigquery.py:1241} INFO - Inserting job ***_data_ingestion_gcs_dag_create_temp_table_task_2025_04_13T00_00_00_00_00_4510745a7ccfe2bb925555d4c1a81572
[2025-04-14T01:26:20.946+0000] {taskinstance.py:3313} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 768, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 734, in _execute_callable
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 424, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 2731, in execute
    job.result(timeout=self.result_timeout, retry=self.result_retry)
  File "/home/airflow/.local/lib/python3.12/site-packages/google/cloud/bigquery/job/query.py", line 1590, in result
    do_get_result()
  File "/home/airflow/.local/lib/python3.12/site-packages/google/cloud/bigquery/job/query.py", line 1579, in do_get_result
    super(QueryJob, self).result(retry=retry, timeout=timeout)
  File "/home/airflow/.local/lib/python3.12/site-packages/google/cloud/bigquery/job/base.py", line 971, in result
    return super(_AsyncJob, self).result(timeout=timeout, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/google/api_core/future/polling.py", line 261, in result
    raise self._exception
google.api_core.exceptions.BadRequest: 400 Syntax error: Expected end of input but got keyword OPTIONS at [10:30]; reason: invalidQuery, location: query, message: Syntax error: Expected end of input but got keyword OPTIONS at [10:30]

Location: US
Job ID: airflow_data_ingestion_gcs_dag_create_temp_table_task_2025_04_13T00_00_00_00_00_4510745a7ccfe2bb925555d4c1a81572

[2025-04-14T01:26:20.973+0000] {logging_mixin.py:190} INFO - Task instance in failure state
[2025-04-14T01:26:20.973+0000] {logging_mixin.py:190} INFO - Task start:2025-04-14 01:26:20.420405+00:00 end:2025-04-14 01:26:20.972175+00:00 duration:0.55177
[2025-04-14T01:26:20.974+0000] {logging_mixin.py:190} INFO - Task:<Task(BigQueryInsertJobOperator): create_temp_table_task> dag:<DAG: data_ingestion_gcs_dag> dagrun:<DagRun data_ingestion_gcs_dag @ 2025-04-13 00:00:00+00:00: scheduled__2025-04-13T00:00:00+00:00, state:running, queued_at: 2025-04-14 01:26:03.127079+00:00. externally triggered: False>
[2025-04-14T01:26:20.975+0000] {logging_mixin.py:190} INFO - Failure caused by 400 Syntax error: Expected end of input but got keyword OPTIONS at [10:30]; reason: invalidQuery, location: query, message: Syntax error: Expected end of input but got keyword OPTIONS at [10:30]

Location: US
Job ID: ***_data_ingestion_gcs_dag_create_temp_table_task_2025_04_13T00_00_00_00_00_4510745a7ccfe2bb925555d4c1a81572
[2025-04-14T01:26:20.975+0000] {taskinstance.py:1226} INFO - Marking task as UP_FOR_RETRY. dag_id=data_ingestion_gcs_dag, task_id=create_temp_table_task, run_id=scheduled__2025-04-13T00:00:00+00:00, execution_date=20250413T000000, start_date=20250414T012620, end_date=20250414T012620
[2025-04-14T01:26:21.025+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-04-14T01:26:21.026+0000] {standard_task_runner.py:124} ERROR - Failed to execute job 7 for task create_temp_table_task (400 Syntax error: Expected end of input but got keyword OPTIONS at [10:30]; reason: invalidQuery, location: query, message: Syntax error: Expected end of input but got keyword OPTIONS at [10:30]

Location: US
Job ID: airflow_data_ingestion_gcs_dag_create_temp_table_task_2025_04_13T00_00_00_00_00_4510745a7ccfe2bb925555d4c1a81572
; 189)
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py", line 117, in _start_by_fork
    ret = args.func(args, dag=self.dag)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/cli.py", line 116, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 483, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 256, in _run_task_by_selected_method
    return _run_raw_task(args, ti)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 341, in _run_raw_task
    return ti._run_raw_task(
           ^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3006, in _run_raw_task
    return _run_raw_task(
           ^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 274, in _run_raw_task
    TaskInstance._execute_task_with_callbacks(
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3161, in _execute_task_with_callbacks
    result = self._execute_task(context, task_orig)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3185, in _execute_task
    return _execute_task(self, context, task_orig)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 768, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 734, in _execute_callable
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 424, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 2731, in execute
    job.result(timeout=self.result_timeout, retry=self.result_retry)
  File "/home/airflow/.local/lib/python3.12/site-packages/google/cloud/bigquery/job/query.py", line 1590, in result
    do_get_result()
  File "/home/airflow/.local/lib/python3.12/site-packages/google/cloud/bigquery/job/query.py", line 1579, in do_get_result
    super(QueryJob, self).result(retry=retry, timeout=timeout)
  File "/home/airflow/.local/lib/python3.12/site-packages/google/cloud/bigquery/job/base.py", line 971, in result
    return super(_AsyncJob, self).result(timeout=timeout, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/google/api_core/future/polling.py", line 261, in result
    raise self._exception
google.api_core.exceptions.BadRequest: 400 Syntax error: Expected end of input but got keyword OPTIONS at [10:30]; reason: invalidQuery, location: query, message: Syntax error: Expected end of input but got keyword OPTIONS at [10:30]

Location: US
Job ID: airflow_data_ingestion_gcs_dag_create_temp_table_task_2025_04_13T00_00_00_00_00_4510745a7ccfe2bb925555d4c1a81572

[2025-04-14T01:26:21.073+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 1
[2025-04-14T01:26:21.085+0000] {taskinstance.py:3901} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-04-14T01:26:21.086+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-04-14T01:29:23.805+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-04-14T01:29:23.822+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: data_ingestion_gcs_dag.create_temp_table_task scheduled__2025-04-13T00:00:00+00:00 [queued]>
[2025-04-14T01:29:23.829+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: data_ingestion_gcs_dag.create_temp_table_task scheduled__2025-04-13T00:00:00+00:00 [queued]>
[2025-04-14T01:29:23.830+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 2
[2025-04-14T01:29:23.835+0000] {taskinstance.py:2890} INFO - Executing <Task(BigQueryInsertJobOperator): create_temp_table_task> on 2025-04-13 00:00:00+00:00
[2025-04-14T01:29:23.840+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=102) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2025-04-14T01:29:23.840+0000] {standard_task_runner.py:72} INFO - Started process 104 to run task
[2025-04-14T01:29:23.841+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'data_ingestion_gcs_dag', 'create_temp_table_task', 'scheduled__2025-04-13T00:00:00+00:00', '--job-id', '7', '--raw', '--subdir', 'DAGS_FOLDER/data_ingestion_gcs_dag.py', '--cfg-path', '/tmp/tmpjk44yffb']
[2025-04-14T01:29:23.841+0000] {standard_task_runner.py:105} INFO - Job 7: Subtask create_temp_table_task
[2025-04-14T01:29:23.866+0000] {task_command.py:467} INFO - Running <TaskInstance: data_ingestion_gcs_dag.create_temp_table_task scheduled__2025-04-13T00:00:00+00:00 [running]> on host 04d41ae960dc
[2025-04-14T01:29:23.903+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='data_ingestion_gcs_dag' AIRFLOW_CTX_TASK_ID='create_temp_table_task' AIRFLOW_CTX_EXECUTION_DATE='2025-04-13T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-04-13T00:00:00+00:00'
[2025-04-14T01:29:23.904+0000] {logging_mixin.py:190} INFO - Task instance is in running state
[2025-04-14T01:29:23.904+0000] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2025-04-14T01:29:23.904+0000] {logging_mixin.py:190} INFO - Current task name:create_temp_table_task state:running start_date:2025-04-14 01:29:23.823161+00:00
[2025-04-14T01:29:23.905+0000] {logging_mixin.py:190} INFO - Dag name:data_ingestion_gcs_dag and current dag run status:running
[2025-04-14T01:29:23.905+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-04-14T01:29:23.906+0000] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2025-04-14T01:29:23.906+0000] {connection.py:277} WARNING - Connection schemes (type: google_cloud_platform) shall not contain '_' according to RFC3986.
[2025-04-14T01:29:23.906+0000] {base.py:84} INFO - Retrieving connection 'google_cloud_default'
[2025-04-14T01:29:23.934+0000] {bigquery.py:2665} INFO - Executing: {'query': {'query': "\n          CREATE OR REPLACE TABLE `nyc-projects-455321.building_permits.external_table`\n          AS\n          SELECT\n            borough,\n            job_type,\n            block,\n            lot,\n            PARSE_TIMESTAMP('%m-%d-%Y', issuance_date) AS issuance_date,\n            CAST(permit_si_no AS INT64) AS permit_si_no,\n            CAST(gis_latitude AS FLOAT64) AS gis_latitude,\n            CAST(gis_longitude AS FLOAT64) AS gis_longitude,\n            gis_nta_name\n          FROM `nyc-projects-455321.building_permits.external_table`;\n        ", 'useLegacySql': False}}'
[2025-04-14T01:29:23.935+0000] {bigquery.py:1241} INFO - Inserting job ***_data_ingestion_gcs_dag_create_temp_table_task_2025_04_13T00_00_00_00_00_d7f5b1a1209102c36bce9bed162eb759
[2025-04-14T01:29:24.847+0000] {taskinstance.py:3313} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 768, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 734, in _execute_callable
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 424, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 2731, in execute
    job.result(timeout=self.result_timeout, retry=self.result_retry)
  File "/home/airflow/.local/lib/python3.12/site-packages/google/cloud/bigquery/job/query.py", line 1590, in result
    do_get_result()
  File "/home/airflow/.local/lib/python3.12/site-packages/google/cloud/bigquery/job/query.py", line 1579, in do_get_result
    super(QueryJob, self).result(retry=retry, timeout=timeout)
  File "/home/airflow/.local/lib/python3.12/site-packages/google/cloud/bigquery/job/base.py", line 971, in result
    return super(_AsyncJob, self).result(timeout=timeout, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/google/api_core/future/polling.py", line 261, in result
    raise self._exception
google.api_core.exceptions.BadRequest: 400 nyc-projects-455321:building_permits.external_table is not allowed for this operation because it currently has type EXTERNAL.; reason: invalid, message: nyc-projects-455321:building_permits.external_table is not allowed for this operation because it currently has type EXTERNAL.

Location: US
Job ID: airflow_data_ingestion_gcs_dag_create_temp_table_task_2025_04_13T00_00_00_00_00_d7f5b1a1209102c36bce9bed162eb759

[2025-04-14T01:29:24.857+0000] {logging_mixin.py:190} INFO - Task instance in failure state
[2025-04-14T01:29:24.857+0000] {logging_mixin.py:190} INFO - Task start:2025-04-14 01:29:23.823161+00:00 end:2025-04-14 01:29:24.856923+00:00 duration:1.033762
[2025-04-14T01:29:24.857+0000] {logging_mixin.py:190} INFO - Task:<Task(BigQueryInsertJobOperator): create_temp_table_task> dag:<DAG: data_ingestion_gcs_dag> dagrun:<DagRun data_ingestion_gcs_dag @ 2025-04-13 00:00:00+00:00: scheduled__2025-04-13T00:00:00+00:00, state:running, queued_at: 2025-04-14 01:29:06.855789+00:00. externally triggered: False>
[2025-04-14T01:29:24.857+0000] {logging_mixin.py:190} INFO - Failure caused by 400 nyc-projects-455321:building_permits.external_table is not allowed for this operation because it currently has type EXTERNAL.; reason: invalid, message: nyc-projects-455321:building_permits.external_table is not allowed for this operation because it currently has type EXTERNAL.

Location: US
Job ID: ***_data_ingestion_gcs_dag_create_temp_table_task_2025_04_13T00_00_00_00_00_d7f5b1a1209102c36bce9bed162eb759
[2025-04-14T01:29:24.858+0000] {taskinstance.py:1226} INFO - Marking task as UP_FOR_RETRY. dag_id=data_ingestion_gcs_dag, task_id=create_temp_table_task, run_id=scheduled__2025-04-13T00:00:00+00:00, execution_date=20250413T000000, start_date=20250414T012923, end_date=20250414T012924
[2025-04-14T01:29:24.878+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-04-14T01:29:24.878+0000] {standard_task_runner.py:124} ERROR - Failed to execute job 7 for task create_temp_table_task (400 nyc-projects-455321:building_permits.external_table is not allowed for this operation because it currently has type EXTERNAL.; reason: invalid, message: nyc-projects-455321:building_permits.external_table is not allowed for this operation because it currently has type EXTERNAL.

Location: US
Job ID: airflow_data_ingestion_gcs_dag_create_temp_table_task_2025_04_13T00_00_00_00_00_d7f5b1a1209102c36bce9bed162eb759
; 104)
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py", line 117, in _start_by_fork
    ret = args.func(args, dag=self.dag)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/cli.py", line 116, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 483, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 256, in _run_task_by_selected_method
    return _run_raw_task(args, ti)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 341, in _run_raw_task
    return ti._run_raw_task(
           ^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3006, in _run_raw_task
    return _run_raw_task(
           ^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 274, in _run_raw_task
    TaskInstance._execute_task_with_callbacks(
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3161, in _execute_task_with_callbacks
    result = self._execute_task(context, task_orig)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3185, in _execute_task
    return _execute_task(self, context, task_orig)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 768, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 734, in _execute_callable
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 424, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 2731, in execute
    job.result(timeout=self.result_timeout, retry=self.result_retry)
  File "/home/airflow/.local/lib/python3.12/site-packages/google/cloud/bigquery/job/query.py", line 1590, in result
    do_get_result()
  File "/home/airflow/.local/lib/python3.12/site-packages/google/cloud/bigquery/job/query.py", line 1579, in do_get_result
    super(QueryJob, self).result(retry=retry, timeout=timeout)
  File "/home/airflow/.local/lib/python3.12/site-packages/google/cloud/bigquery/job/base.py", line 971, in result
    return super(_AsyncJob, self).result(timeout=timeout, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/google/api_core/future/polling.py", line 261, in result
    raise self._exception
google.api_core.exceptions.BadRequest: 400 nyc-projects-455321:building_permits.external_table is not allowed for this operation because it currently has type EXTERNAL.; reason: invalid, message: nyc-projects-455321:building_permits.external_table is not allowed for this operation because it currently has type EXTERNAL.

Location: US
Job ID: airflow_data_ingestion_gcs_dag_create_temp_table_task_2025_04_13T00_00_00_00_00_d7f5b1a1209102c36bce9bed162eb759

[2025-04-14T01:29:24.923+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 1
[2025-04-14T01:29:24.943+0000] {taskinstance.py:3901} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-04-14T01:29:24.944+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-04-14T01:48:27.298+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-04-14T01:48:27.309+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: data_ingestion_gcs_dag.create_temp_table_task scheduled__2025-04-13T00:00:00+00:00 [queued]>
[2025-04-14T01:48:27.314+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: data_ingestion_gcs_dag.create_temp_table_task scheduled__2025-04-13T00:00:00+00:00 [queued]>
[2025-04-14T01:48:27.314+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 2
[2025-04-14T01:48:27.320+0000] {taskinstance.py:2890} INFO - Executing <Task(BigQueryInsertJobOperator): create_temp_table_task> on 2025-04-13 00:00:00+00:00
[2025-04-14T01:48:27.324+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=116) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2025-04-14T01:48:27.325+0000] {standard_task_runner.py:72} INFO - Started process 118 to run task
[2025-04-14T01:48:27.325+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'data_ingestion_gcs_dag', 'create_temp_table_task', 'scheduled__2025-04-13T00:00:00+00:00', '--job-id', '7', '--raw', '--subdir', 'DAGS_FOLDER/data_ingestion_gcs_dag.py', '--cfg-path', '/tmp/tmp60e_1e4x']
[2025-04-14T01:48:27.326+0000] {standard_task_runner.py:105} INFO - Job 7: Subtask create_temp_table_task
[2025-04-14T01:48:27.379+0000] {task_command.py:467} INFO - Running <TaskInstance: data_ingestion_gcs_dag.create_temp_table_task scheduled__2025-04-13T00:00:00+00:00 [running]> on host f8826741f369
[2025-04-14T01:48:27.440+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='data_ingestion_gcs_dag' AIRFLOW_CTX_TASK_ID='create_temp_table_task' AIRFLOW_CTX_EXECUTION_DATE='2025-04-13T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-04-13T00:00:00+00:00'
[2025-04-14T01:48:27.443+0000] {logging_mixin.py:190} INFO - Task instance is in running state
[2025-04-14T01:48:27.444+0000] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2025-04-14T01:48:27.444+0000] {logging_mixin.py:190} INFO - Current task name:create_temp_table_task state:running start_date:2025-04-14 01:48:27.309565+00:00
[2025-04-14T01:48:27.444+0000] {logging_mixin.py:190} INFO - Dag name:data_ingestion_gcs_dag and current dag run status:running
[2025-04-14T01:48:27.446+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-04-14T01:48:27.447+0000] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2025-04-14T01:48:27.447+0000] {connection.py:277} WARNING - Connection schemes (type: google_cloud_platform) shall not contain '_' according to RFC3986.
[2025-04-14T01:48:27.448+0000] {base.py:84} INFO - Retrieving connection 'google_cloud_default'
[2025-04-14T01:48:27.481+0000] {bigquery.py:2665} INFO - Executing: {'query': {'query': "\n          CREATE OR REPLACE TABLE `nyc-projects-455321.building_permits.external_table`\n          AS\n          SELECT\n            borough,\n            job_type,\n            block,\n            lot,\n            PARSE_TIMESTAMP('%m/%d/%Y', issuance_date) AS issuance_date,\n            CAST(permit_si_no AS INT64) AS permit_si_no,\n            CAST(gis_latitude AS FLOAT64) AS gis_latitude,\n            CAST(gis_longitude AS FLOAT64) AS gis_longitude,\n            gis_nta_name\n          FROM `nyc-projects-455321.building_permits.external_table`;\n        ", 'useLegacySql': False}}'
[2025-04-14T01:48:27.482+0000] {bigquery.py:1241} INFO - Inserting job ***_data_ingestion_gcs_dag_create_temp_table_task_2025_04_13T00_00_00_00_00_b0fb1d428790194945e05733c8058e0d
[2025-04-14T01:48:28.618+0000] {taskinstance.py:3313} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 768, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 734, in _execute_callable
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 424, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 2731, in execute
    job.result(timeout=self.result_timeout, retry=self.result_retry)
  File "/home/airflow/.local/lib/python3.12/site-packages/google/cloud/bigquery/job/query.py", line 1590, in result
    do_get_result()
  File "/home/airflow/.local/lib/python3.12/site-packages/google/cloud/bigquery/job/query.py", line 1579, in do_get_result
    super(QueryJob, self).result(retry=retry, timeout=timeout)
  File "/home/airflow/.local/lib/python3.12/site-packages/google/cloud/bigquery/job/base.py", line 971, in result
    return super(_AsyncJob, self).result(timeout=timeout, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/google/api_core/future/polling.py", line 261, in result
    raise self._exception
google.api_core.exceptions.BadRequest: 400 nyc-projects-455321:building_permits.external_table is not allowed for this operation because it currently has type EXTERNAL.; reason: invalid, message: nyc-projects-455321:building_permits.external_table is not allowed for this operation because it currently has type EXTERNAL.

Location: US
Job ID: airflow_data_ingestion_gcs_dag_create_temp_table_task_2025_04_13T00_00_00_00_00_b0fb1d428790194945e05733c8058e0d

[2025-04-14T01:48:28.655+0000] {logging_mixin.py:190} INFO - Task instance in failure state
[2025-04-14T01:48:28.655+0000] {logging_mixin.py:190} INFO - Task start:2025-04-14 01:48:27.309565+00:00 end:2025-04-14 01:48:28.654042+00:00 duration:1.344477
[2025-04-14T01:48:28.656+0000] {logging_mixin.py:190} INFO - Task:<Task(BigQueryInsertJobOperator): create_temp_table_task> dag:<DAG: data_ingestion_gcs_dag> dagrun:<DagRun data_ingestion_gcs_dag @ 2025-04-13 00:00:00+00:00: scheduled__2025-04-13T00:00:00+00:00, state:running, queued_at: 2025-04-14 01:48:09.247077+00:00. externally triggered: False>
[2025-04-14T01:48:28.657+0000] {logging_mixin.py:190} INFO - Failure caused by 400 nyc-projects-455321:building_permits.external_table is not allowed for this operation because it currently has type EXTERNAL.; reason: invalid, message: nyc-projects-455321:building_permits.external_table is not allowed for this operation because it currently has type EXTERNAL.

Location: US
Job ID: ***_data_ingestion_gcs_dag_create_temp_table_task_2025_04_13T00_00_00_00_00_b0fb1d428790194945e05733c8058e0d
[2025-04-14T01:48:28.658+0000] {taskinstance.py:1226} INFO - Marking task as UP_FOR_RETRY. dag_id=data_ingestion_gcs_dag, task_id=create_temp_table_task, run_id=scheduled__2025-04-13T00:00:00+00:00, execution_date=20250413T000000, start_date=20250414T014827, end_date=20250414T014828
[2025-04-14T01:48:28.716+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-04-14T01:48:28.717+0000] {standard_task_runner.py:124} ERROR - Failed to execute job 7 for task create_temp_table_task (400 nyc-projects-455321:building_permits.external_table is not allowed for this operation because it currently has type EXTERNAL.; reason: invalid, message: nyc-projects-455321:building_permits.external_table is not allowed for this operation because it currently has type EXTERNAL.

Location: US
Job ID: airflow_data_ingestion_gcs_dag_create_temp_table_task_2025_04_13T00_00_00_00_00_b0fb1d428790194945e05733c8058e0d
; 118)
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py", line 117, in _start_by_fork
    ret = args.func(args, dag=self.dag)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/cli.py", line 116, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 483, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 256, in _run_task_by_selected_method
    return _run_raw_task(args, ti)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 341, in _run_raw_task
    return ti._run_raw_task(
           ^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3006, in _run_raw_task
    return _run_raw_task(
           ^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 274, in _run_raw_task
    TaskInstance._execute_task_with_callbacks(
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3161, in _execute_task_with_callbacks
    result = self._execute_task(context, task_orig)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3185, in _execute_task
    return _execute_task(self, context, task_orig)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 768, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 734, in _execute_callable
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 424, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 2731, in execute
    job.result(timeout=self.result_timeout, retry=self.result_retry)
  File "/home/airflow/.local/lib/python3.12/site-packages/google/cloud/bigquery/job/query.py", line 1590, in result
    do_get_result()
  File "/home/airflow/.local/lib/python3.12/site-packages/google/cloud/bigquery/job/query.py", line 1579, in do_get_result
    super(QueryJob, self).result(retry=retry, timeout=timeout)
  File "/home/airflow/.local/lib/python3.12/site-packages/google/cloud/bigquery/job/base.py", line 971, in result
    return super(_AsyncJob, self).result(timeout=timeout, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/google/api_core/future/polling.py", line 261, in result
    raise self._exception
google.api_core.exceptions.BadRequest: 400 nyc-projects-455321:building_permits.external_table is not allowed for this operation because it currently has type EXTERNAL.; reason: invalid, message: nyc-projects-455321:building_permits.external_table is not allowed for this operation because it currently has type EXTERNAL.

Location: US
Job ID: airflow_data_ingestion_gcs_dag_create_temp_table_task_2025_04_13T00_00_00_00_00_b0fb1d428790194945e05733c8058e0d

[2025-04-14T01:48:28.735+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 1
[2025-04-14T01:48:28.766+0000] {taskinstance.py:3901} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-04-14T01:48:28.769+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-04-14T02:01:56.427+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-04-14T02:01:56.441+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: data_ingestion_gcs_dag.create_temp_table_task scheduled__2025-04-13T00:00:00+00:00 [queued]>
[2025-04-14T02:01:56.446+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: data_ingestion_gcs_dag.create_temp_table_task scheduled__2025-04-13T00:00:00+00:00 [queued]>
[2025-04-14T02:01:56.447+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 2
[2025-04-14T02:01:56.453+0000] {taskinstance.py:2890} INFO - Executing <Task(BigQueryInsertJobOperator): create_temp_table_task> on 2025-04-13 00:00:00+00:00
[2025-04-14T02:01:56.457+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=106) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2025-04-14T02:01:56.458+0000] {standard_task_runner.py:72} INFO - Started process 108 to run task
[2025-04-14T02:01:56.459+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'data_ingestion_gcs_dag', 'create_temp_table_task', 'scheduled__2025-04-13T00:00:00+00:00', '--job-id', '8', '--raw', '--subdir', 'DAGS_FOLDER/data_ingestion_gcs_dag.py', '--cfg-path', '/tmp/tmp1cmxk11u']
[2025-04-14T02:01:56.460+0000] {standard_task_runner.py:105} INFO - Job 8: Subtask create_temp_table_task
[2025-04-14T02:01:56.487+0000] {task_command.py:467} INFO - Running <TaskInstance: data_ingestion_gcs_dag.create_temp_table_task scheduled__2025-04-13T00:00:00+00:00 [running]> on host 20ea3f936809
[2025-04-14T02:01:56.536+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='data_ingestion_gcs_dag' AIRFLOW_CTX_TASK_ID='create_temp_table_task' AIRFLOW_CTX_EXECUTION_DATE='2025-04-13T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-04-13T00:00:00+00:00'
[2025-04-14T02:01:56.537+0000] {logging_mixin.py:190} INFO - Task instance is in running state
[2025-04-14T02:01:56.537+0000] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2025-04-14T02:01:56.537+0000] {logging_mixin.py:190} INFO - Current task name:create_temp_table_task state:running start_date:2025-04-14 02:01:56.441642+00:00
[2025-04-14T02:01:56.537+0000] {logging_mixin.py:190} INFO - Dag name:data_ingestion_gcs_dag and current dag run status:running
[2025-04-14T02:01:56.537+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-04-14T02:01:56.538+0000] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2025-04-14T02:01:56.539+0000] {connection.py:277} WARNING - Connection schemes (type: google_cloud_platform) shall not contain '_' according to RFC3986.
[2025-04-14T02:01:56.539+0000] {base.py:84} INFO - Retrieving connection 'google_cloud_default'
[2025-04-14T02:01:56.568+0000] {bigquery.py:2665} INFO - Executing: {'query': {'query': "\n          CREATE OR REPLACE TABLE `nyc-projects-455321.building_permits.external_table`\n          AS\n          SELECT\n            borough,\n            job_type,\n            block,\n            lot,\n            PARSE_TIMESTAMP('%m/%d/%Y', issuance_date) AS issuance_date,\n            CAST(permit_si_no AS INT64) AS permit_si_no,\n            CAST(gis_latitude AS FLOAT64) AS gis_latitude,\n            CAST(gis_longitude AS FLOAT64) AS gis_longitude,\n            gis_nta_name\n          FROM `nyc-projects-455321.building_permits.external_table`;\n        ", 'useLegacySql': False}}'
[2025-04-14T02:01:56.569+0000] {bigquery.py:1241} INFO - Inserting job ***_data_ingestion_gcs_dag_create_temp_table_task_2025_04_13T00_00_00_00_00_7873bad99ae160e4df0ad13aeea4d480
[2025-04-14T02:01:57.417+0000] {taskinstance.py:3313} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 768, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 734, in _execute_callable
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 424, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 2731, in execute
    job.result(timeout=self.result_timeout, retry=self.result_retry)
  File "/home/airflow/.local/lib/python3.12/site-packages/google/cloud/bigquery/job/query.py", line 1590, in result
    do_get_result()
  File "/home/airflow/.local/lib/python3.12/site-packages/google/cloud/bigquery/job/query.py", line 1579, in do_get_result
    super(QueryJob, self).result(retry=retry, timeout=timeout)
  File "/home/airflow/.local/lib/python3.12/site-packages/google/cloud/bigquery/job/base.py", line 971, in result
    return super(_AsyncJob, self).result(timeout=timeout, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/google/api_core/future/polling.py", line 261, in result
    raise self._exception
google.api_core.exceptions.BadRequest: 400 nyc-projects-455321:building_permits.external_table is not allowed for this operation because it currently has type EXTERNAL.; reason: invalid, message: nyc-projects-455321:building_permits.external_table is not allowed for this operation because it currently has type EXTERNAL.

Location: US
Job ID: airflow_data_ingestion_gcs_dag_create_temp_table_task_2025_04_13T00_00_00_00_00_7873bad99ae160e4df0ad13aeea4d480

[2025-04-14T02:01:57.430+0000] {logging_mixin.py:190} INFO - Task instance in failure state
[2025-04-14T02:01:57.430+0000] {logging_mixin.py:190} INFO - Task start:2025-04-14 02:01:56.441642+00:00 end:2025-04-14 02:01:57.430003+00:00 duration:0.988361
[2025-04-14T02:01:57.430+0000] {logging_mixin.py:190} INFO - Task:<Task(BigQueryInsertJobOperator): create_temp_table_task> dag:<DAG: data_ingestion_gcs_dag> dagrun:<DagRun data_ingestion_gcs_dag @ 2025-04-13 00:00:00+00:00: scheduled__2025-04-13T00:00:00+00:00, state:running, queued_at: 2025-04-14 02:01:38.169098+00:00. externally triggered: False>
[2025-04-14T02:01:57.430+0000] {logging_mixin.py:190} INFO - Failure caused by 400 nyc-projects-455321:building_permits.external_table is not allowed for this operation because it currently has type EXTERNAL.; reason: invalid, message: nyc-projects-455321:building_permits.external_table is not allowed for this operation because it currently has type EXTERNAL.

Location: US
Job ID: ***_data_ingestion_gcs_dag_create_temp_table_task_2025_04_13T00_00_00_00_00_7873bad99ae160e4df0ad13aeea4d480
[2025-04-14T02:01:57.431+0000] {taskinstance.py:1226} INFO - Marking task as UP_FOR_RETRY. dag_id=data_ingestion_gcs_dag, task_id=create_temp_table_task, run_id=scheduled__2025-04-13T00:00:00+00:00, execution_date=20250413T000000, start_date=20250414T020156, end_date=20250414T020157
[2025-04-14T02:01:57.448+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-04-14T02:01:57.449+0000] {standard_task_runner.py:124} ERROR - Failed to execute job 8 for task create_temp_table_task (400 nyc-projects-455321:building_permits.external_table is not allowed for this operation because it currently has type EXTERNAL.; reason: invalid, message: nyc-projects-455321:building_permits.external_table is not allowed for this operation because it currently has type EXTERNAL.

Location: US
Job ID: airflow_data_ingestion_gcs_dag_create_temp_table_task_2025_04_13T00_00_00_00_00_7873bad99ae160e4df0ad13aeea4d480
; 108)
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py", line 117, in _start_by_fork
    ret = args.func(args, dag=self.dag)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/cli.py", line 116, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 483, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 256, in _run_task_by_selected_method
    return _run_raw_task(args, ti)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 341, in _run_raw_task
    return ti._run_raw_task(
           ^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3006, in _run_raw_task
    return _run_raw_task(
           ^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 274, in _run_raw_task
    TaskInstance._execute_task_with_callbacks(
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3161, in _execute_task_with_callbacks
    result = self._execute_task(context, task_orig)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3185, in _execute_task
    return _execute_task(self, context, task_orig)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 768, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 734, in _execute_callable
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 424, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 2731, in execute
    job.result(timeout=self.result_timeout, retry=self.result_retry)
  File "/home/airflow/.local/lib/python3.12/site-packages/google/cloud/bigquery/job/query.py", line 1590, in result
    do_get_result()
  File "/home/airflow/.local/lib/python3.12/site-packages/google/cloud/bigquery/job/query.py", line 1579, in do_get_result
    super(QueryJob, self).result(retry=retry, timeout=timeout)
  File "/home/airflow/.local/lib/python3.12/site-packages/google/cloud/bigquery/job/base.py", line 971, in result
    return super(_AsyncJob, self).result(timeout=timeout, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/google/api_core/future/polling.py", line 261, in result
    raise self._exception
google.api_core.exceptions.BadRequest: 400 nyc-projects-455321:building_permits.external_table is not allowed for this operation because it currently has type EXTERNAL.; reason: invalid, message: nyc-projects-455321:building_permits.external_table is not allowed for this operation because it currently has type EXTERNAL.

Location: US
Job ID: airflow_data_ingestion_gcs_dag_create_temp_table_task_2025_04_13T00_00_00_00_00_7873bad99ae160e4df0ad13aeea4d480

[2025-04-14T02:01:57.498+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 1
[2025-04-14T02:01:57.515+0000] {taskinstance.py:3901} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-04-14T02:01:57.516+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-04-14T02:04:24.377+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-04-14T02:04:24.387+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: data_ingestion_gcs_dag.create_temp_table_task scheduled__2025-04-13T00:00:00+00:00 [queued]>
[2025-04-14T02:04:24.392+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: data_ingestion_gcs_dag.create_temp_table_task scheduled__2025-04-13T00:00:00+00:00 [queued]>
[2025-04-14T02:04:24.392+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 2
[2025-04-14T02:04:24.398+0000] {taskinstance.py:2890} INFO - Executing <Task(BigQueryInsertJobOperator): create_temp_table_task> on 2025-04-13 00:00:00+00:00
[2025-04-14T02:04:24.402+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=109) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2025-04-14T02:04:24.403+0000] {standard_task_runner.py:72} INFO - Started process 111 to run task
[2025-04-14T02:04:24.405+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'data_ingestion_gcs_dag', 'create_temp_table_task', 'scheduled__2025-04-13T00:00:00+00:00', '--job-id', '7', '--raw', '--subdir', 'DAGS_FOLDER/data_ingestion_gcs_dag.py', '--cfg-path', '/tmp/tmpjxtrt4dd']
[2025-04-14T02:04:24.406+0000] {standard_task_runner.py:105} INFO - Job 7: Subtask create_temp_table_task
[2025-04-14T02:04:24.429+0000] {task_command.py:467} INFO - Running <TaskInstance: data_ingestion_gcs_dag.create_temp_table_task scheduled__2025-04-13T00:00:00+00:00 [running]> on host c92133bf2a65
[2025-04-14T02:04:24.470+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='data_ingestion_gcs_dag' AIRFLOW_CTX_TASK_ID='create_temp_table_task' AIRFLOW_CTX_EXECUTION_DATE='2025-04-13T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-04-13T00:00:00+00:00'
[2025-04-14T02:04:24.471+0000] {logging_mixin.py:190} INFO - Task instance is in running state
[2025-04-14T02:04:24.471+0000] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2025-04-14T02:04:24.472+0000] {logging_mixin.py:190} INFO - Current task name:create_temp_table_task state:running start_date:2025-04-14 02:04:24.387579+00:00
[2025-04-14T02:04:24.472+0000] {logging_mixin.py:190} INFO - Dag name:data_ingestion_gcs_dag and current dag run status:running
[2025-04-14T02:04:24.472+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-04-14T02:04:24.473+0000] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2025-04-14T02:04:24.473+0000] {connection.py:277} WARNING - Connection schemes (type: google_cloud_platform) shall not contain '_' according to RFC3986.
[2025-04-14T02:04:24.473+0000] {base.py:84} INFO - Retrieving connection 'google_cloud_default'
[2025-04-14T02:04:24.501+0000] {bigquery.py:2665} INFO - Executing: {'query': {'query': '\n          CREATE OR REPLACE TABLE `nyc-projects-455321.building_permits.external_table`\n          AS\n          SELECT\n            borough,\n            job_type,\n            block,\n            lot,\n            issuance_date,\n            CAST(permit_si_no AS INT64) AS permit_si_no,\n            CAST(gis_latitude AS FLOAT64) AS gis_latitude,\n            CAST(gis_longitude AS FLOAT64) AS gis_longitude,\n            gis_nta_name\n          FROM `nyc-projects-455321.building_permits.external_table`;\n        ', 'useLegacySql': False}}'
[2025-04-14T02:04:24.502+0000] {bigquery.py:1241} INFO - Inserting job ***_data_ingestion_gcs_dag_create_temp_table_task_2025_04_13T00_00_00_00_00_d66f8927310363a5b14a089ad00fae5d
[2025-04-14T02:04:25.360+0000] {taskinstance.py:3313} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 768, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 734, in _execute_callable
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 424, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 2731, in execute
    job.result(timeout=self.result_timeout, retry=self.result_retry)
  File "/home/airflow/.local/lib/python3.12/site-packages/google/cloud/bigquery/job/query.py", line 1590, in result
    do_get_result()
  File "/home/airflow/.local/lib/python3.12/site-packages/google/cloud/bigquery/job/query.py", line 1579, in do_get_result
    super(QueryJob, self).result(retry=retry, timeout=timeout)
  File "/home/airflow/.local/lib/python3.12/site-packages/google/cloud/bigquery/job/base.py", line 971, in result
    return super(_AsyncJob, self).result(timeout=timeout, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/google/api_core/future/polling.py", line 261, in result
    raise self._exception
google.api_core.exceptions.BadRequest: 400 nyc-projects-455321:building_permits.external_table is not allowed for this operation because it currently has type EXTERNAL.; reason: invalid, message: nyc-projects-455321:building_permits.external_table is not allowed for this operation because it currently has type EXTERNAL.

Location: US
Job ID: airflow_data_ingestion_gcs_dag_create_temp_table_task_2025_04_13T00_00_00_00_00_d66f8927310363a5b14a089ad00fae5d

[2025-04-14T02:04:25.386+0000] {logging_mixin.py:190} INFO - Task instance in failure state
[2025-04-14T02:04:25.387+0000] {logging_mixin.py:190} INFO - Task start:2025-04-14 02:04:24.387579+00:00 end:2025-04-14 02:04:25.385779+00:00 duration:0.9982
[2025-04-14T02:04:25.388+0000] {logging_mixin.py:190} INFO - Task:<Task(BigQueryInsertJobOperator): create_temp_table_task> dag:<DAG: data_ingestion_gcs_dag> dagrun:<DagRun data_ingestion_gcs_dag @ 2025-04-13 00:00:00+00:00: scheduled__2025-04-13T00:00:00+00:00, state:running, queued_at: 2025-04-14 02:04:05.330070+00:00. externally triggered: False>
[2025-04-14T02:04:25.389+0000] {logging_mixin.py:190} INFO - Failure caused by 400 nyc-projects-455321:building_permits.external_table is not allowed for this operation because it currently has type EXTERNAL.; reason: invalid, message: nyc-projects-455321:building_permits.external_table is not allowed for this operation because it currently has type EXTERNAL.

Location: US
Job ID: ***_data_ingestion_gcs_dag_create_temp_table_task_2025_04_13T00_00_00_00_00_d66f8927310363a5b14a089ad00fae5d
[2025-04-14T02:04:25.390+0000] {taskinstance.py:1226} INFO - Marking task as UP_FOR_RETRY. dag_id=data_ingestion_gcs_dag, task_id=create_temp_table_task, run_id=scheduled__2025-04-13T00:00:00+00:00, execution_date=20250413T000000, start_date=20250414T020424, end_date=20250414T020425
[2025-04-14T02:04:25.431+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-04-14T02:04:25.432+0000] {standard_task_runner.py:124} ERROR - Failed to execute job 7 for task create_temp_table_task (400 nyc-projects-455321:building_permits.external_table is not allowed for this operation because it currently has type EXTERNAL.; reason: invalid, message: nyc-projects-455321:building_permits.external_table is not allowed for this operation because it currently has type EXTERNAL.

Location: US
Job ID: airflow_data_ingestion_gcs_dag_create_temp_table_task_2025_04_13T00_00_00_00_00_d66f8927310363a5b14a089ad00fae5d
; 111)
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py", line 117, in _start_by_fork
    ret = args.func(args, dag=self.dag)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/cli.py", line 116, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 483, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 256, in _run_task_by_selected_method
    return _run_raw_task(args, ti)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 341, in _run_raw_task
    return ti._run_raw_task(
           ^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3006, in _run_raw_task
    return _run_raw_task(
           ^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 274, in _run_raw_task
    TaskInstance._execute_task_with_callbacks(
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3161, in _execute_task_with_callbacks
    result = self._execute_task(context, task_orig)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3185, in _execute_task
    return _execute_task(self, context, task_orig)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 768, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 734, in _execute_callable
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 424, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 2731, in execute
    job.result(timeout=self.result_timeout, retry=self.result_retry)
  File "/home/airflow/.local/lib/python3.12/site-packages/google/cloud/bigquery/job/query.py", line 1590, in result
    do_get_result()
  File "/home/airflow/.local/lib/python3.12/site-packages/google/cloud/bigquery/job/query.py", line 1579, in do_get_result
    super(QueryJob, self).result(retry=retry, timeout=timeout)
  File "/home/airflow/.local/lib/python3.12/site-packages/google/cloud/bigquery/job/base.py", line 971, in result
    return super(_AsyncJob, self).result(timeout=timeout, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/google/api_core/future/polling.py", line 261, in result
    raise self._exception
google.api_core.exceptions.BadRequest: 400 nyc-projects-455321:building_permits.external_table is not allowed for this operation because it currently has type EXTERNAL.; reason: invalid, message: nyc-projects-455321:building_permits.external_table is not allowed for this operation because it currently has type EXTERNAL.

Location: US
Job ID: airflow_data_ingestion_gcs_dag_create_temp_table_task_2025_04_13T00_00_00_00_00_d66f8927310363a5b14a089ad00fae5d

[2025-04-14T02:04:25.446+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 1
[2025-04-14T02:04:25.466+0000] {taskinstance.py:3901} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-04-14T02:04:25.467+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-04-14T02:06:49.331+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-04-14T02:06:49.341+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: data_ingestion_gcs_dag.create_temp_table_task scheduled__2025-04-13T00:00:00+00:00 [queued]>
[2025-04-14T02:06:49.347+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: data_ingestion_gcs_dag.create_temp_table_task scheduled__2025-04-13T00:00:00+00:00 [queued]>
[2025-04-14T02:06:49.347+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 2
[2025-04-14T02:06:49.353+0000] {taskinstance.py:2890} INFO - Executing <Task(BigQueryInsertJobOperator): create_temp_table_task> on 2025-04-13 00:00:00+00:00
[2025-04-14T02:06:49.357+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=102) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2025-04-14T02:06:49.358+0000] {standard_task_runner.py:72} INFO - Started process 104 to run task
[2025-04-14T02:06:49.359+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'data_ingestion_gcs_dag', 'create_temp_table_task', 'scheduled__2025-04-13T00:00:00+00:00', '--job-id', '7', '--raw', '--subdir', 'DAGS_FOLDER/data_ingestion_gcs_dag.py', '--cfg-path', '/tmp/tmpzdsxntee']
[2025-04-14T02:06:49.359+0000] {standard_task_runner.py:105} INFO - Job 7: Subtask create_temp_table_task
[2025-04-14T02:06:49.386+0000] {task_command.py:467} INFO - Running <TaskInstance: data_ingestion_gcs_dag.create_temp_table_task scheduled__2025-04-13T00:00:00+00:00 [running]> on host 8a29e6f3f045
[2025-04-14T02:06:49.424+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='data_ingestion_gcs_dag' AIRFLOW_CTX_TASK_ID='create_temp_table_task' AIRFLOW_CTX_EXECUTION_DATE='2025-04-13T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-04-13T00:00:00+00:00'
[2025-04-14T02:06:49.425+0000] {logging_mixin.py:190} INFO - Task instance is in running state
[2025-04-14T02:06:49.425+0000] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2025-04-14T02:06:49.425+0000] {logging_mixin.py:190} INFO - Current task name:create_temp_table_task state:running start_date:2025-04-14 02:06:49.342200+00:00
[2025-04-14T02:06:49.426+0000] {logging_mixin.py:190} INFO - Dag name:data_ingestion_gcs_dag and current dag run status:running
[2025-04-14T02:06:49.426+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-04-14T02:06:49.427+0000] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2025-04-14T02:06:49.427+0000] {connection.py:277} WARNING - Connection schemes (type: google_cloud_platform) shall not contain '_' according to RFC3986.
[2025-04-14T02:06:49.427+0000] {base.py:84} INFO - Retrieving connection 'google_cloud_default'
[2025-04-14T02:06:49.456+0000] {bigquery.py:2665} INFO - Executing: {'query': {'query': '\n          CREATE OR REPLACE TABLE `nyc-projects-455321.building_permits.external_table`\n          AS\n          SELECT\n            borough,\n            job_type,\n            block,\n            lot,\n            issuance_date,\n            permit_si_no,\n            gis_latitude,\n            gis_longitude,\n            gis_nta_name\n          FROM `nyc-projects-455321.building_permits.external_table`;\n        ', 'useLegacySql': False}}'
[2025-04-14T02:06:49.457+0000] {bigquery.py:1241} INFO - Inserting job ***_data_ingestion_gcs_dag_create_temp_table_task_2025_04_13T00_00_00_00_00_2f24382fb49d969523611ad6ec72414d
[2025-04-14T02:06:50.333+0000] {taskinstance.py:3313} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 768, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 734, in _execute_callable
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 424, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 2731, in execute
    job.result(timeout=self.result_timeout, retry=self.result_retry)
  File "/home/airflow/.local/lib/python3.12/site-packages/google/cloud/bigquery/job/query.py", line 1590, in result
    do_get_result()
  File "/home/airflow/.local/lib/python3.12/site-packages/google/cloud/bigquery/job/query.py", line 1579, in do_get_result
    super(QueryJob, self).result(retry=retry, timeout=timeout)
  File "/home/airflow/.local/lib/python3.12/site-packages/google/cloud/bigquery/job/base.py", line 971, in result
    return super(_AsyncJob, self).result(timeout=timeout, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/google/api_core/future/polling.py", line 261, in result
    raise self._exception
google.api_core.exceptions.BadRequest: 400 nyc-projects-455321:building_permits.external_table is not allowed for this operation because it currently has type EXTERNAL.; reason: invalid, message: nyc-projects-455321:building_permits.external_table is not allowed for this operation because it currently has type EXTERNAL.

Location: US
Job ID: airflow_data_ingestion_gcs_dag_create_temp_table_task_2025_04_13T00_00_00_00_00_2f24382fb49d969523611ad6ec72414d

[2025-04-14T02:06:50.351+0000] {logging_mixin.py:190} INFO - Task instance in failure state
[2025-04-14T02:06:50.351+0000] {logging_mixin.py:190} INFO - Task start:2025-04-14 02:06:49.342200+00:00 end:2025-04-14 02:06:50.350344+00:00 duration:1.008144
[2025-04-14T02:06:50.352+0000] {logging_mixin.py:190} INFO - Task:<Task(BigQueryInsertJobOperator): create_temp_table_task> dag:<DAG: data_ingestion_gcs_dag> dagrun:<DagRun data_ingestion_gcs_dag @ 2025-04-13 00:00:00+00:00: scheduled__2025-04-13T00:00:00+00:00, state:running, queued_at: 2025-04-14 02:06:31.082673+00:00. externally triggered: False>
[2025-04-14T02:06:50.352+0000] {logging_mixin.py:190} INFO - Failure caused by 400 nyc-projects-455321:building_permits.external_table is not allowed for this operation because it currently has type EXTERNAL.; reason: invalid, message: nyc-projects-455321:building_permits.external_table is not allowed for this operation because it currently has type EXTERNAL.

Location: US
Job ID: ***_data_ingestion_gcs_dag_create_temp_table_task_2025_04_13T00_00_00_00_00_2f24382fb49d969523611ad6ec72414d
[2025-04-14T02:06:50.352+0000] {taskinstance.py:1226} INFO - Marking task as UP_FOR_RETRY. dag_id=data_ingestion_gcs_dag, task_id=create_temp_table_task, run_id=scheduled__2025-04-13T00:00:00+00:00, execution_date=20250413T000000, start_date=20250414T020649, end_date=20250414T020650
[2025-04-14T02:06:50.373+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-04-14T02:06:50.373+0000] {standard_task_runner.py:124} ERROR - Failed to execute job 7 for task create_temp_table_task (400 nyc-projects-455321:building_permits.external_table is not allowed for this operation because it currently has type EXTERNAL.; reason: invalid, message: nyc-projects-455321:building_permits.external_table is not allowed for this operation because it currently has type EXTERNAL.

Location: US
Job ID: airflow_data_ingestion_gcs_dag_create_temp_table_task_2025_04_13T00_00_00_00_00_2f24382fb49d969523611ad6ec72414d
; 104)
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py", line 117, in _start_by_fork
    ret = args.func(args, dag=self.dag)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/cli.py", line 116, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 483, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 256, in _run_task_by_selected_method
    return _run_raw_task(args, ti)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 341, in _run_raw_task
    return ti._run_raw_task(
           ^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3006, in _run_raw_task
    return _run_raw_task(
           ^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 274, in _run_raw_task
    TaskInstance._execute_task_with_callbacks(
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3161, in _execute_task_with_callbacks
    result = self._execute_task(context, task_orig)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3185, in _execute_task
    return _execute_task(self, context, task_orig)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 768, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 734, in _execute_callable
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 424, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 2731, in execute
    job.result(timeout=self.result_timeout, retry=self.result_retry)
  File "/home/airflow/.local/lib/python3.12/site-packages/google/cloud/bigquery/job/query.py", line 1590, in result
    do_get_result()
  File "/home/airflow/.local/lib/python3.12/site-packages/google/cloud/bigquery/job/query.py", line 1579, in do_get_result
    super(QueryJob, self).result(retry=retry, timeout=timeout)
  File "/home/airflow/.local/lib/python3.12/site-packages/google/cloud/bigquery/job/base.py", line 971, in result
    return super(_AsyncJob, self).result(timeout=timeout, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/google/api_core/future/polling.py", line 261, in result
    raise self._exception
google.api_core.exceptions.BadRequest: 400 nyc-projects-455321:building_permits.external_table is not allowed for this operation because it currently has type EXTERNAL.; reason: invalid, message: nyc-projects-455321:building_permits.external_table is not allowed for this operation because it currently has type EXTERNAL.

Location: US
Job ID: airflow_data_ingestion_gcs_dag_create_temp_table_task_2025_04_13T00_00_00_00_00_2f24382fb49d969523611ad6ec72414d

[2025-04-14T02:06:50.401+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 1
[2025-04-14T02:06:50.429+0000] {taskinstance.py:3901} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-04-14T02:06:50.430+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-04-14T02:10:50.092+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-04-14T02:10:50.102+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: data_ingestion_gcs_dag.create_temp_table_task scheduled__2025-04-13T00:00:00+00:00 [queued]>
[2025-04-14T02:10:50.108+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: data_ingestion_gcs_dag.create_temp_table_task scheduled__2025-04-13T00:00:00+00:00 [queued]>
[2025-04-14T02:10:50.108+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 2
[2025-04-14T02:10:50.115+0000] {taskinstance.py:2890} INFO - Executing <Task(BigQueryInsertJobOperator): create_temp_table_task> on 2025-04-13 00:00:00+00:00
[2025-04-14T02:10:50.119+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=95) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2025-04-14T02:10:50.120+0000] {standard_task_runner.py:72} INFO - Started process 97 to run task
[2025-04-14T02:10:50.120+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'data_ingestion_gcs_dag', 'create_temp_table_task', 'scheduled__2025-04-13T00:00:00+00:00', '--job-id', '7', '--raw', '--subdir', 'DAGS_FOLDER/data_ingestion_gcs_dag.py', '--cfg-path', '/tmp/tmpkx4mfs80']
[2025-04-14T02:10:50.121+0000] {standard_task_runner.py:105} INFO - Job 7: Subtask create_temp_table_task
[2025-04-14T02:10:50.148+0000] {task_command.py:467} INFO - Running <TaskInstance: data_ingestion_gcs_dag.create_temp_table_task scheduled__2025-04-13T00:00:00+00:00 [running]> on host 73dbef184f29
[2025-04-14T02:10:50.185+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='data_ingestion_gcs_dag' AIRFLOW_CTX_TASK_ID='create_temp_table_task' AIRFLOW_CTX_EXECUTION_DATE='2025-04-13T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-04-13T00:00:00+00:00'
[2025-04-14T02:10:50.187+0000] {logging_mixin.py:190} INFO - Task instance is in running state
[2025-04-14T02:10:50.187+0000] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2025-04-14T02:10:50.187+0000] {logging_mixin.py:190} INFO - Current task name:create_temp_table_task state:running start_date:2025-04-14 02:10:50.103127+00:00
[2025-04-14T02:10:50.188+0000] {logging_mixin.py:190} INFO - Dag name:data_ingestion_gcs_dag and current dag run status:running
[2025-04-14T02:10:50.188+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-04-14T02:10:50.189+0000] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2025-04-14T02:10:50.190+0000] {connection.py:277} WARNING - Connection schemes (type: google_cloud_platform) shall not contain '_' according to RFC3986.
[2025-04-14T02:10:50.191+0000] {base.py:84} INFO - Retrieving connection 'google_cloud_default'
[2025-04-14T02:10:50.219+0000] {bigquery.py:2665} INFO - Executing: {'query': {'query': '\n          CREATE TABLE `nyc-projects-455321.building_permits.external_table`\n          AS\n          SELECT\n            borough,\n            job_type,\n            block,\n            lot,\n            issuance_date,\n            permit_si_no,\n            gis_latitude,\n            gis_longitude,\n            gis_nta_name\n          FROM `nyc-projects-455321.building_permits.external_table`;\n        ', 'useLegacySql': False}}'
[2025-04-14T02:10:50.220+0000] {bigquery.py:1241} INFO - Inserting job ***_data_ingestion_gcs_dag_create_temp_table_task_2025_04_13T00_00_00_00_00_ea17f0d3d6c48d91dc0219822f9605b9
[2025-04-14T02:10:51.040+0000] {taskinstance.py:3313} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 768, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 734, in _execute_callable
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 424, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 2731, in execute
    job.result(timeout=self.result_timeout, retry=self.result_retry)
  File "/home/airflow/.local/lib/python3.12/site-packages/google/cloud/bigquery/job/query.py", line 1590, in result
    do_get_result()
  File "/home/airflow/.local/lib/python3.12/site-packages/google/cloud/bigquery/job/query.py", line 1579, in do_get_result
    super(QueryJob, self).result(retry=retry, timeout=timeout)
  File "/home/airflow/.local/lib/python3.12/site-packages/google/cloud/bigquery/job/base.py", line 971, in result
    return super(_AsyncJob, self).result(timeout=timeout, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/google/api_core/future/polling.py", line 261, in result
    raise self._exception
google.api_core.exceptions.Conflict: 409 Already Exists: Table nyc-projects-455321:building_permits.external_table; reason: duplicate, message: Already Exists: Table nyc-projects-455321:building_permits.external_table

Location: US
Job ID: airflow_data_ingestion_gcs_dag_create_temp_table_task_2025_04_13T00_00_00_00_00_ea17f0d3d6c48d91dc0219822f9605b9

[2025-04-14T02:10:51.050+0000] {logging_mixin.py:190} INFO - Task instance in failure state
[2025-04-14T02:10:51.051+0000] {logging_mixin.py:190} INFO - Task start:2025-04-14 02:10:50.103127+00:00 end:2025-04-14 02:10:51.050366+00:00 duration:0.947239
[2025-04-14T02:10:51.051+0000] {logging_mixin.py:190} INFO - Task:<Task(BigQueryInsertJobOperator): create_temp_table_task> dag:<DAG: data_ingestion_gcs_dag> dagrun:<DagRun data_ingestion_gcs_dag @ 2025-04-13 00:00:00+00:00: scheduled__2025-04-13T00:00:00+00:00, state:running, queued_at: 2025-04-14 02:10:32.078205+00:00. externally triggered: False>
[2025-04-14T02:10:51.051+0000] {logging_mixin.py:190} INFO - Failure caused by 409 Already Exists: Table nyc-projects-455321:building_permits.external_table; reason: duplicate, message: Already Exists: Table nyc-projects-455321:building_permits.external_table

Location: US
Job ID: ***_data_ingestion_gcs_dag_create_temp_table_task_2025_04_13T00_00_00_00_00_ea17f0d3d6c48d91dc0219822f9605b9
[2025-04-14T02:10:51.051+0000] {taskinstance.py:1226} INFO - Marking task as UP_FOR_RETRY. dag_id=data_ingestion_gcs_dag, task_id=create_temp_table_task, run_id=scheduled__2025-04-13T00:00:00+00:00, execution_date=20250413T000000, start_date=20250414T021050, end_date=20250414T021051
[2025-04-14T02:10:51.073+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-04-14T02:10:51.073+0000] {standard_task_runner.py:124} ERROR - Failed to execute job 7 for task create_temp_table_task (409 Already Exists: Table nyc-projects-455321:building_permits.external_table; reason: duplicate, message: Already Exists: Table nyc-projects-455321:building_permits.external_table

Location: US
Job ID: airflow_data_ingestion_gcs_dag_create_temp_table_task_2025_04_13T00_00_00_00_00_ea17f0d3d6c48d91dc0219822f9605b9
; 97)
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py", line 117, in _start_by_fork
    ret = args.func(args, dag=self.dag)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/cli.py", line 116, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 483, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 256, in _run_task_by_selected_method
    return _run_raw_task(args, ti)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 341, in _run_raw_task
    return ti._run_raw_task(
           ^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3006, in _run_raw_task
    return _run_raw_task(
           ^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 274, in _run_raw_task
    TaskInstance._execute_task_with_callbacks(
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3161, in _execute_task_with_callbacks
    result = self._execute_task(context, task_orig)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3185, in _execute_task
    return _execute_task(self, context, task_orig)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 768, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 734, in _execute_callable
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 424, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 2731, in execute
    job.result(timeout=self.result_timeout, retry=self.result_retry)
  File "/home/airflow/.local/lib/python3.12/site-packages/google/cloud/bigquery/job/query.py", line 1590, in result
    do_get_result()
  File "/home/airflow/.local/lib/python3.12/site-packages/google/cloud/bigquery/job/query.py", line 1579, in do_get_result
    super(QueryJob, self).result(retry=retry, timeout=timeout)
  File "/home/airflow/.local/lib/python3.12/site-packages/google/cloud/bigquery/job/base.py", line 971, in result
    return super(_AsyncJob, self).result(timeout=timeout, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/google/api_core/future/polling.py", line 261, in result
    raise self._exception
google.api_core.exceptions.Conflict: 409 Already Exists: Table nyc-projects-455321:building_permits.external_table; reason: duplicate, message: Already Exists: Table nyc-projects-455321:building_permits.external_table

Location: US
Job ID: airflow_data_ingestion_gcs_dag_create_temp_table_task_2025_04_13T00_00_00_00_00_ea17f0d3d6c48d91dc0219822f9605b9

[2025-04-14T02:10:51.122+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 1
[2025-04-14T02:10:51.142+0000] {taskinstance.py:3901} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-04-14T02:10:51.146+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-04-14T02:13:03.034+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-04-14T02:13:03.047+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: data_ingestion_gcs_dag.create_temp_table_task scheduled__2025-04-13T00:00:00+00:00 [queued]>
[2025-04-14T02:13:03.055+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: data_ingestion_gcs_dag.create_temp_table_task scheduled__2025-04-13T00:00:00+00:00 [queued]>
[2025-04-14T02:13:03.055+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 2
[2025-04-14T02:13:03.062+0000] {taskinstance.py:2890} INFO - Executing <Task(BigQueryInsertJobOperator): create_temp_table_task> on 2025-04-13 00:00:00+00:00
[2025-04-14T02:13:03.066+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=95) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2025-04-14T02:13:03.066+0000] {standard_task_runner.py:72} INFO - Started process 104 to run task
[2025-04-14T02:13:03.067+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'data_ingestion_gcs_dag', 'create_temp_table_task', 'scheduled__2025-04-13T00:00:00+00:00', '--job-id', '7', '--raw', '--subdir', 'DAGS_FOLDER/data_ingestion_gcs_dag.py', '--cfg-path', '/tmp/tmp6yi_ct2m']
[2025-04-14T02:13:03.068+0000] {standard_task_runner.py:105} INFO - Job 7: Subtask create_temp_table_task
[2025-04-14T02:13:03.096+0000] {task_command.py:467} INFO - Running <TaskInstance: data_ingestion_gcs_dag.create_temp_table_task scheduled__2025-04-13T00:00:00+00:00 [running]> on host dae9419049da
[2025-04-14T02:13:03.133+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='data_ingestion_gcs_dag' AIRFLOW_CTX_TASK_ID='create_temp_table_task' AIRFLOW_CTX_EXECUTION_DATE='2025-04-13T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-04-13T00:00:00+00:00'
[2025-04-14T02:13:03.134+0000] {logging_mixin.py:190} INFO - Task instance is in running state
[2025-04-14T02:13:03.135+0000] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2025-04-14T02:13:03.135+0000] {logging_mixin.py:190} INFO - Current task name:create_temp_table_task state:running start_date:2025-04-14 02:13:03.047430+00:00
[2025-04-14T02:13:03.135+0000] {logging_mixin.py:190} INFO - Dag name:data_ingestion_gcs_dag and current dag run status:running
[2025-04-14T02:13:03.135+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-04-14T02:13:03.136+0000] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2025-04-14T02:13:03.136+0000] {connection.py:277} WARNING - Connection schemes (type: google_cloud_platform) shall not contain '_' according to RFC3986.
[2025-04-14T02:13:03.137+0000] {base.py:84} INFO - Retrieving connection 'google_cloud_default'
[2025-04-14T02:13:03.165+0000] {bigquery.py:2665} INFO - Executing: {'query': {'query': '\n          DROP TABLE IF EXISTS `nyc-projects-455321.building_permits.external_table`;\n          CREATE TABLE `nyc-projects-455321.building_permits.external_table`\n          AS\n          SELECT\n            borough,\n            job_type,\n            block,\n            lot,\n            issuance_date,\n            permit_si_no,\n            gis_latitude,\n            gis_longitude,\n            gis_nta_name\n          FROM `nyc-projects-455321.building_permits.external_table`;\n        ', 'useLegacySql': False}}'
[2025-04-14T02:13:03.166+0000] {bigquery.py:1241} INFO - Inserting job ***_data_ingestion_gcs_dag_create_temp_table_task_2025_04_13T00_00_00_00_00_86d762cfddfbfc0a8286d197c9d2e185
[2025-04-14T02:13:04.517+0000] {taskinstance.py:3313} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 768, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 734, in _execute_callable
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 424, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 2731, in execute
    job.result(timeout=self.result_timeout, retry=self.result_retry)
  File "/home/airflow/.local/lib/python3.12/site-packages/google/cloud/bigquery/job/query.py", line 1590, in result
    do_get_result()
  File "/home/airflow/.local/lib/python3.12/site-packages/google/cloud/bigquery/job/query.py", line 1579, in do_get_result
    super(QueryJob, self).result(retry=retry, timeout=timeout)
  File "/home/airflow/.local/lib/python3.12/site-packages/google/cloud/bigquery/job/base.py", line 971, in result
    return super(_AsyncJob, self).result(timeout=timeout, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/google/api_core/future/polling.py", line 261, in result
    raise self._exception
google.api_core.exceptions.BadRequest: 400 Not found: Table nyc-projects-455321:building_permits.external_table was not found in location US at [3:11]

Location: US
Job ID: airflow_data_ingestion_gcs_dag_create_temp_table_task_2025_04_13T00_00_00_00_00_86d762cfddfbfc0a8286d197c9d2e185

[2025-04-14T02:13:04.527+0000] {logging_mixin.py:190} INFO - Task instance in failure state
[2025-04-14T02:13:04.528+0000] {logging_mixin.py:190} INFO - Task start:2025-04-14 02:13:03.047430+00:00 end:2025-04-14 02:13:04.527666+00:00 duration:1.480236
[2025-04-14T02:13:04.528+0000] {logging_mixin.py:190} INFO - Task:<Task(BigQueryInsertJobOperator): create_temp_table_task> dag:<DAG: data_ingestion_gcs_dag> dagrun:<DagRun data_ingestion_gcs_dag @ 2025-04-13 00:00:00+00:00: scheduled__2025-04-13T00:00:00+00:00, state:running, queued_at: 2025-04-14 02:12:45.675355+00:00. externally triggered: False>
[2025-04-14T02:13:04.528+0000] {logging_mixin.py:190} INFO - Failure caused by 400 Not found: Table nyc-projects-455321:building_permits.external_table was not found in location US at [3:11]

Location: US
Job ID: ***_data_ingestion_gcs_dag_create_temp_table_task_2025_04_13T00_00_00_00_00_86d762cfddfbfc0a8286d197c9d2e185
[2025-04-14T02:13:04.528+0000] {taskinstance.py:1226} INFO - Marking task as UP_FOR_RETRY. dag_id=data_ingestion_gcs_dag, task_id=create_temp_table_task, run_id=scheduled__2025-04-13T00:00:00+00:00, execution_date=20250413T000000, start_date=20250414T021303, end_date=20250414T021304
[2025-04-14T02:13:04.546+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-04-14T02:13:04.546+0000] {standard_task_runner.py:124} ERROR - Failed to execute job 7 for task create_temp_table_task (400 Not found: Table nyc-projects-455321:building_permits.external_table was not found in location US at [3:11]

Location: US
Job ID: airflow_data_ingestion_gcs_dag_create_temp_table_task_2025_04_13T00_00_00_00_00_86d762cfddfbfc0a8286d197c9d2e185
; 104)
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py", line 117, in _start_by_fork
    ret = args.func(args, dag=self.dag)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/cli.py", line 116, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 483, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 256, in _run_task_by_selected_method
    return _run_raw_task(args, ti)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 341, in _run_raw_task
    return ti._run_raw_task(
           ^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3006, in _run_raw_task
    return _run_raw_task(
           ^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 274, in _run_raw_task
    TaskInstance._execute_task_with_callbacks(
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3161, in _execute_task_with_callbacks
    result = self._execute_task(context, task_orig)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3185, in _execute_task
    return _execute_task(self, context, task_orig)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 768, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 734, in _execute_callable
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 424, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 2731, in execute
    job.result(timeout=self.result_timeout, retry=self.result_retry)
  File "/home/airflow/.local/lib/python3.12/site-packages/google/cloud/bigquery/job/query.py", line 1590, in result
    do_get_result()
  File "/home/airflow/.local/lib/python3.12/site-packages/google/cloud/bigquery/job/query.py", line 1579, in do_get_result
    super(QueryJob, self).result(retry=retry, timeout=timeout)
  File "/home/airflow/.local/lib/python3.12/site-packages/google/cloud/bigquery/job/base.py", line 971, in result
    return super(_AsyncJob, self).result(timeout=timeout, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/google/api_core/future/polling.py", line 261, in result
    raise self._exception
google.api_core.exceptions.BadRequest: 400 Not found: Table nyc-projects-455321:building_permits.external_table was not found in location US at [3:11]

Location: US
Job ID: airflow_data_ingestion_gcs_dag_create_temp_table_task_2025_04_13T00_00_00_00_00_86d762cfddfbfc0a8286d197c9d2e185

[2025-04-14T02:13:04.558+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 1
[2025-04-14T02:13:04.571+0000] {taskinstance.py:3901} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-04-14T02:13:04.572+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-04-14T02:16:19.298+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-04-14T02:16:19.310+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: data_ingestion_gcs_dag.create_temp_table_task scheduled__2025-04-13T00:00:00+00:00 [queued]>
[2025-04-14T02:16:19.315+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: data_ingestion_gcs_dag.create_temp_table_task scheduled__2025-04-13T00:00:00+00:00 [queued]>
[2025-04-14T02:16:19.316+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 2
[2025-04-14T02:16:19.323+0000] {taskinstance.py:2890} INFO - Executing <Task(BigQueryInsertJobOperator): create_temp_table_task> on 2025-04-13 00:00:00+00:00
[2025-04-14T02:16:19.327+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=95) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2025-04-14T02:16:19.328+0000] {standard_task_runner.py:72} INFO - Started process 97 to run task
[2025-04-14T02:16:19.330+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'data_ingestion_gcs_dag', 'create_temp_table_task', 'scheduled__2025-04-13T00:00:00+00:00', '--job-id', '7', '--raw', '--subdir', 'DAGS_FOLDER/data_ingestion_gcs_dag.py', '--cfg-path', '/tmp/tmpcoguldm8']
[2025-04-14T02:16:19.331+0000] {standard_task_runner.py:105} INFO - Job 7: Subtask create_temp_table_task
[2025-04-14T02:16:19.357+0000] {task_command.py:467} INFO - Running <TaskInstance: data_ingestion_gcs_dag.create_temp_table_task scheduled__2025-04-13T00:00:00+00:00 [running]> on host f2d268f0f33a
[2025-04-14T02:16:19.394+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='data_ingestion_gcs_dag' AIRFLOW_CTX_TASK_ID='create_temp_table_task' AIRFLOW_CTX_EXECUTION_DATE='2025-04-13T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-04-13T00:00:00+00:00'
[2025-04-14T02:16:19.395+0000] {logging_mixin.py:190} INFO - Task instance is in running state
[2025-04-14T02:16:19.395+0000] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2025-04-14T02:16:19.395+0000] {logging_mixin.py:190} INFO - Current task name:create_temp_table_task state:running start_date:2025-04-14 02:16:19.310453+00:00
[2025-04-14T02:16:19.395+0000] {logging_mixin.py:190} INFO - Dag name:data_ingestion_gcs_dag and current dag run status:running
[2025-04-14T02:16:19.395+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-04-14T02:16:19.396+0000] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2025-04-14T02:16:19.396+0000] {connection.py:277} WARNING - Connection schemes (type: google_cloud_platform) shall not contain '_' according to RFC3986.
[2025-04-14T02:16:19.397+0000] {base.py:84} INFO - Retrieving connection 'google_cloud_default'
[2025-04-14T02:16:19.425+0000] {bigquery.py:2665} INFO - Executing: {'query': {'query': "\n          CREATE OR REPLACE TABLE `nyc-projects-455321.building_permits.temp_table`\n          AS\n          SELECT\n            borough,\n            job_type,\n            block,\n            lot,\n            PARSE_TIMESTAMP('%m/%d/%Y', issuance_date) AS issuance_date,\n            CAST(permit_si_no AS INT64) AS permit_si_no,\n            CAST(gis_latitude AS FLOAT64) AS gis_latitude,\n            CAST(gis_longitude AS FLOAT64) AS gis_longitude,\n            gis_nta_name\n          FROM `nyc-projects-455321.building_permits.external_table`;\n        ", 'useLegacySql': False}}'
[2025-04-14T02:16:19.426+0000] {bigquery.py:1241} INFO - Inserting job ***_data_ingestion_gcs_dag_create_temp_table_task_2025_04_13T00_00_00_00_00_f0dfc65859bf0bf123086cd1e86c79b1
[2025-04-14T02:16:22.204+0000] {bigquery.py:2636} INFO - Job ***_data_ingestion_gcs_dag_create_temp_table_task_2025_04_13T00_00_00_00_00_f0dfc65859bf0bf123086cd1e86c79b1 is completed. Checking the job status
[2025-04-14T02:16:22.240+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-04-14T02:16:22.241+0000] {taskinstance.py:353} INFO - Marking task as SUCCESS. dag_id=data_ingestion_gcs_dag, task_id=create_temp_table_task, run_id=scheduled__2025-04-13T00:00:00+00:00, execution_date=20250413T000000, start_date=20250414T021619, end_date=20250414T021622
[2025-04-14T02:16:22.262+0000] {logging_mixin.py:190} INFO - Task instance in success state
[2025-04-14T02:16:22.262+0000] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2025-04-14T02:16:22.262+0000] {logging_mixin.py:190} INFO - Dag name:data_ingestion_gcs_dag queued_at:2025-04-14 02:16:04.289451+00:00
[2025-04-14T02:16:22.262+0000] {logging_mixin.py:190} INFO - Task hostname:f2d268f0f33a operator:BigQueryInsertJobOperator
[2025-04-14T02:16:22.300+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2025-04-14T02:16:22.327+0000] {taskinstance.py:3901} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-04-14T02:16:22.328+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-04-14T03:50:02.019+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-04-14T03:50:02.030+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: data_ingestion_gcs_dag.create_temp_table_task scheduled__2025-04-13T00:00:00+00:00 [queued]>
[2025-04-14T03:50:02.035+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: data_ingestion_gcs_dag.create_temp_table_task scheduled__2025-04-13T00:00:00+00:00 [queued]>
[2025-04-14T03:50:02.036+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 2
[2025-04-14T03:50:02.042+0000] {taskinstance.py:2890} INFO - Executing <Task(BigQueryInsertJobOperator): create_temp_table_task> on 2025-04-13 00:00:00+00:00
[2025-04-14T03:50:02.046+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=134) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2025-04-14T03:50:02.048+0000] {standard_task_runner.py:72} INFO - Started process 136 to run task
[2025-04-14T03:50:02.048+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'data_ingestion_gcs_dag', 'create_temp_table_task', 'scheduled__2025-04-13T00:00:00+00:00', '--job-id', '8', '--raw', '--subdir', 'DAGS_FOLDER/data_ingestion_gcs_dag.py', '--cfg-path', '/tmp/tmpp3tyjeqv']
[2025-04-14T03:50:02.049+0000] {standard_task_runner.py:105} INFO - Job 8: Subtask create_temp_table_task
[2025-04-14T03:50:02.073+0000] {task_command.py:467} INFO - Running <TaskInstance: data_ingestion_gcs_dag.create_temp_table_task scheduled__2025-04-13T00:00:00+00:00 [running]> on host f7261677844c
[2025-04-14T03:50:02.127+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='data_ingestion_gcs_dag' AIRFLOW_CTX_TASK_ID='create_temp_table_task' AIRFLOW_CTX_EXECUTION_DATE='2025-04-13T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-04-13T00:00:00+00:00'
[2025-04-14T03:50:02.130+0000] {logging_mixin.py:190} INFO - Task instance is in running state
[2025-04-14T03:50:02.130+0000] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2025-04-14T03:50:02.131+0000] {logging_mixin.py:190} INFO - Current task name:create_temp_table_task state:running start_date:2025-04-14 03:50:02.030660+00:00
[2025-04-14T03:50:02.131+0000] {logging_mixin.py:190} INFO - Dag name:data_ingestion_gcs_dag and current dag run status:running
[2025-04-14T03:50:02.131+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-04-14T03:50:02.133+0000] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2025-04-14T03:50:02.134+0000] {connection.py:277} WARNING - Connection schemes (type: google_cloud_platform) shall not contain '_' according to RFC3986.
[2025-04-14T03:50:02.134+0000] {base.py:84} INFO - Retrieving connection 'google_cloud_default'
[2025-04-14T03:50:02.171+0000] {bigquery.py:2665} INFO - Executing: {'query': {'query': "\n          CREATE OR REPLACE TABLE `nyc-projects-455321.building_permits.temp_table`\n          AS\n          SELECT\n            borough,\n            job_type,\n            block,\n            lot,\n            PARSE_TIMESTAMP('%m/%d/%Y', issuance_date) AS issuance_date,\n            CAST(permit_si_no AS INT64) AS permit_si_no,\n            CAST(gis_latitude AS FLOAT64) AS gis_latitude,\n            CAST(gis_longitude AS FLOAT64) AS gis_longitude,\n            gis_nta_name\n          FROM `nyc-projects-455321.building_permits.external_table`;\n        ", 'useLegacySql': False}}'
[2025-04-14T03:50:02.171+0000] {bigquery.py:1241} INFO - Inserting job ***_data_ingestion_gcs_dag_create_temp_table_task_2025_04_13T00_00_00_00_00_9f70376570bd7ce34ea2a48db8647635
[2025-04-14T03:50:05.247+0000] {bigquery.py:2636} INFO - Job ***_data_ingestion_gcs_dag_create_temp_table_task_2025_04_13T00_00_00_00_00_9f70376570bd7ce34ea2a48db8647635 is completed. Checking the job status
[2025-04-14T03:50:05.297+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-04-14T03:50:05.299+0000] {taskinstance.py:353} INFO - Marking task as SUCCESS. dag_id=data_ingestion_gcs_dag, task_id=create_temp_table_task, run_id=scheduled__2025-04-13T00:00:00+00:00, execution_date=20250413T000000, start_date=20250414T035002, end_date=20250414T035005
[2025-04-14T03:50:05.352+0000] {logging_mixin.py:190} INFO - Task instance in success state
[2025-04-14T03:50:05.353+0000] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2025-04-14T03:50:05.354+0000] {logging_mixin.py:190} INFO - Dag name:data_ingestion_gcs_dag queued_at:2025-04-14 03:49:43.644379+00:00
[2025-04-14T03:50:05.355+0000] {logging_mixin.py:190} INFO - Task hostname:f7261677844c operator:BigQueryInsertJobOperator
[2025-04-14T03:50:05.389+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2025-04-14T03:50:05.432+0000] {taskinstance.py:3901} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2025-04-14T03:50:05.433+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-04-14T03:57:50.932+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-04-14T03:57:50.942+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: data_ingestion_gcs_dag.create_temp_table_task scheduled__2025-04-13T00:00:00+00:00 [queued]>
[2025-04-14T03:57:50.948+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: data_ingestion_gcs_dag.create_temp_table_task scheduled__2025-04-13T00:00:00+00:00 [queued]>
[2025-04-14T03:57:50.948+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 2
[2025-04-14T03:57:50.954+0000] {taskinstance.py:2890} INFO - Executing <Task(BigQueryInsertJobOperator): create_temp_table_task> on 2025-04-13 00:00:00+00:00
[2025-04-14T03:57:50.958+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=147) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2025-04-14T03:57:50.960+0000] {standard_task_runner.py:72} INFO - Started process 149 to run task
[2025-04-14T03:57:50.960+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'data_ingestion_gcs_dag', 'create_temp_table_task', 'scheduled__2025-04-13T00:00:00+00:00', '--job-id', '8', '--raw', '--subdir', 'DAGS_FOLDER/data_ingestion_gcs_dag.py', '--cfg-path', '/tmp/tmp6izy6z7r']
[2025-04-14T03:57:50.961+0000] {standard_task_runner.py:105} INFO - Job 8: Subtask create_temp_table_task
[2025-04-14T03:57:50.986+0000] {task_command.py:467} INFO - Running <TaskInstance: data_ingestion_gcs_dag.create_temp_table_task scheduled__2025-04-13T00:00:00+00:00 [running]> on host 3029200567a0
[2025-04-14T03:57:51.023+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='data_ingestion_gcs_dag' AIRFLOW_CTX_TASK_ID='create_temp_table_task' AIRFLOW_CTX_EXECUTION_DATE='2025-04-13T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-04-13T00:00:00+00:00'
[2025-04-14T03:57:51.025+0000] {logging_mixin.py:190} INFO - Task instance is in running state
[2025-04-14T03:57:51.026+0000] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2025-04-14T03:57:51.026+0000] {logging_mixin.py:190} INFO - Current task name:create_temp_table_task state:running start_date:2025-04-14 03:57:50.943065+00:00
[2025-04-14T03:57:51.026+0000] {logging_mixin.py:190} INFO - Dag name:data_ingestion_gcs_dag and current dag run status:running
[2025-04-14T03:57:51.026+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-04-14T03:57:51.028+0000] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2025-04-14T03:57:51.028+0000] {connection.py:277} WARNING - Connection schemes (type: google_cloud_platform) shall not contain '_' according to RFC3986.
[2025-04-14T03:57:51.029+0000] {base.py:84} INFO - Retrieving connection 'google_cloud_default'
[2025-04-14T03:57:51.057+0000] {bigquery.py:2665} INFO - Executing: {'query': {'query': "\n          CREATE OR REPLACE TABLE `nyc-projects-455321.building_permits.temp_table`\n          AS\n          SELECT\n            borough,\n            job_type,\n            block,\n            lot,\n            PARSE_TIMESTAMP('%m/%d/%Y', issuance_date) AS issuance_date,\n            CAST(permit_si_no AS INT64) AS permit_si_no,\n            CAST(gis_latitude AS FLOAT64) AS gis_latitude,\n            CAST(gis_longitude AS FLOAT64) AS gis_longitude,\n            gis_nta_name\n          FROM `nyc-projects-455321.building_permits.external_table`;\n        ", 'useLegacySql': False}}'
[2025-04-14T03:57:51.058+0000] {bigquery.py:1241} INFO - Inserting job ***_data_ingestion_gcs_dag_create_temp_table_task_2025_04_13T00_00_00_00_00_664523487ffca6b099eb702ba8aa6cf3
[2025-04-14T03:57:53.543+0000] {bigquery.py:2636} INFO - Job ***_data_ingestion_gcs_dag_create_temp_table_task_2025_04_13T00_00_00_00_00_664523487ffca6b099eb702ba8aa6cf3 is completed. Checking the job status
[2025-04-14T03:57:53.575+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-04-14T03:57:53.576+0000] {taskinstance.py:353} INFO - Marking task as SUCCESS. dag_id=data_ingestion_gcs_dag, task_id=create_temp_table_task, run_id=scheduled__2025-04-13T00:00:00+00:00, execution_date=20250413T000000, start_date=20250414T035750, end_date=20250414T035753
[2025-04-14T03:57:53.588+0000] {logging_mixin.py:190} INFO - Task instance in success state
[2025-04-14T03:57:53.588+0000] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2025-04-14T03:57:53.588+0000] {logging_mixin.py:190} INFO - Dag name:data_ingestion_gcs_dag queued_at:2025-04-14 03:57:32.901326+00:00
[2025-04-14T03:57:53.589+0000] {logging_mixin.py:190} INFO - Task hostname:3029200567a0 operator:BigQueryInsertJobOperator
[2025-04-14T03:57:53.601+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2025-04-14T03:57:53.625+0000] {taskinstance.py:3901} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2025-04-14T03:57:53.626+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-04-14T04:14:47.740+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-04-14T04:14:47.751+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: data_ingestion_gcs_dag.create_temp_table_task scheduled__2025-04-13T00:00:00+00:00 [queued]>
[2025-04-14T04:14:47.756+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: data_ingestion_gcs_dag.create_temp_table_task scheduled__2025-04-13T00:00:00+00:00 [queued]>
[2025-04-14T04:14:47.757+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 2
[2025-04-14T04:14:47.763+0000] {taskinstance.py:2890} INFO - Executing <Task(BigQueryInsertJobOperator): create_temp_table_task> on 2025-04-13 00:00:00+00:00
[2025-04-14T04:14:47.767+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=106) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2025-04-14T04:14:47.768+0000] {standard_task_runner.py:72} INFO - Started process 108 to run task
[2025-04-14T04:14:47.770+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'data_ingestion_gcs_dag', 'create_temp_table_task', 'scheduled__2025-04-13T00:00:00+00:00', '--job-id', '8', '--raw', '--subdir', 'DAGS_FOLDER/data_ingestion_gcs_dag.py', '--cfg-path', '/tmp/tmpn06cabn7']
[2025-04-14T04:14:47.770+0000] {standard_task_runner.py:105} INFO - Job 8: Subtask create_temp_table_task
[2025-04-14T04:14:47.795+0000] {task_command.py:467} INFO - Running <TaskInstance: data_ingestion_gcs_dag.create_temp_table_task scheduled__2025-04-13T00:00:00+00:00 [running]> on host e86210162b75
[2025-04-14T04:14:47.833+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='data_ingestion_gcs_dag' AIRFLOW_CTX_TASK_ID='create_temp_table_task' AIRFLOW_CTX_EXECUTION_DATE='2025-04-13T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-04-13T00:00:00+00:00'
[2025-04-14T04:14:47.834+0000] {logging_mixin.py:190} INFO - Task instance is in running state
[2025-04-14T04:14:47.834+0000] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2025-04-14T04:14:47.834+0000] {logging_mixin.py:190} INFO - Current task name:create_temp_table_task state:running start_date:2025-04-14 04:14:47.751903+00:00
[2025-04-14T04:14:47.834+0000] {logging_mixin.py:190} INFO - Dag name:data_ingestion_gcs_dag and current dag run status:running
[2025-04-14T04:14:47.834+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-04-14T04:14:47.835+0000] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2025-04-14T04:14:47.835+0000] {connection.py:277} WARNING - Connection schemes (type: google_cloud_platform) shall not contain '_' according to RFC3986.
[2025-04-14T04:14:47.836+0000] {base.py:84} INFO - Retrieving connection 'google_cloud_default'
[2025-04-14T04:14:47.864+0000] {bigquery.py:2665} INFO - Executing: {'query': {'query': "\n          CREATE OR REPLACE TABLE `nyc-projects-455321.building_permits.temp_table`\n          AS\n          SELECT\n            borough,\n            job_type,\n            block,\n            lot,\n            PARSE_TIMESTAMP('%m/%d/%Y', issuance_date) AS issuance_date,\n            CAST(permit_si_no AS INT64) AS permit_si_no,\n            CONCAT(gis_latitude,',',gis_longitude)\n            gis_nta_name\n          FROM `nyc-projects-455321.building_permits.external_table`;\n        ", 'useLegacySql': False}}'
[2025-04-14T04:14:47.865+0000] {bigquery.py:1241} INFO - Inserting job ***_data_ingestion_gcs_dag_create_temp_table_task_2025_04_13T00_00_00_00_00_ecbf6c4c4b5a9a189e6959e7aa58345c
[2025-04-14T04:14:50.454+0000] {bigquery.py:2636} INFO - Job ***_data_ingestion_gcs_dag_create_temp_table_task_2025_04_13T00_00_00_00_00_ecbf6c4c4b5a9a189e6959e7aa58345c is completed. Checking the job status
[2025-04-14T04:14:50.495+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-04-14T04:14:50.496+0000] {taskinstance.py:353} INFO - Marking task as SUCCESS. dag_id=data_ingestion_gcs_dag, task_id=create_temp_table_task, run_id=scheduled__2025-04-13T00:00:00+00:00, execution_date=20250413T000000, start_date=20250414T041447, end_date=20250414T041450
[2025-04-14T04:14:50.522+0000] {logging_mixin.py:190} INFO - Task instance in success state
[2025-04-14T04:14:50.523+0000] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2025-04-14T04:14:50.523+0000] {logging_mixin.py:190} INFO - Dag name:data_ingestion_gcs_dag queued_at:2025-04-14 04:14:28.657183+00:00
[2025-04-14T04:14:50.523+0000] {logging_mixin.py:190} INFO - Task hostname:e86210162b75 operator:BigQueryInsertJobOperator
[2025-04-14T04:14:50.534+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2025-04-14T04:14:50.549+0000] {taskinstance.py:3901} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2025-04-14T04:14:50.550+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-04-14T04:16:59.442+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-04-14T04:16:59.453+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: data_ingestion_gcs_dag.create_temp_table_task scheduled__2025-04-13T00:00:00+00:00 [queued]>
[2025-04-14T04:16:59.458+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: data_ingestion_gcs_dag.create_temp_table_task scheduled__2025-04-13T00:00:00+00:00 [queued]>
[2025-04-14T04:16:59.458+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 2
[2025-04-14T04:16:59.464+0000] {taskinstance.py:2890} INFO - Executing <Task(BigQueryInsertJobOperator): create_temp_table_task> on 2025-04-13 00:00:00+00:00
[2025-04-14T04:16:59.468+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=106) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2025-04-14T04:16:59.469+0000] {standard_task_runner.py:72} INFO - Started process 108 to run task
[2025-04-14T04:16:59.469+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'data_ingestion_gcs_dag', 'create_temp_table_task', 'scheduled__2025-04-13T00:00:00+00:00', '--job-id', '8', '--raw', '--subdir', 'DAGS_FOLDER/data_ingestion_gcs_dag.py', '--cfg-path', '/tmp/tmp9cpaztxo']
[2025-04-14T04:16:59.470+0000] {standard_task_runner.py:105} INFO - Job 8: Subtask create_temp_table_task
[2025-04-14T04:16:59.494+0000] {task_command.py:467} INFO - Running <TaskInstance: data_ingestion_gcs_dag.create_temp_table_task scheduled__2025-04-13T00:00:00+00:00 [running]> on host d054e5caec0f
[2025-04-14T04:16:59.531+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='data_ingestion_gcs_dag' AIRFLOW_CTX_TASK_ID='create_temp_table_task' AIRFLOW_CTX_EXECUTION_DATE='2025-04-13T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-04-13T00:00:00+00:00'
[2025-04-14T04:16:59.532+0000] {logging_mixin.py:190} INFO - Task instance is in running state
[2025-04-14T04:16:59.533+0000] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2025-04-14T04:16:59.533+0000] {logging_mixin.py:190} INFO - Current task name:create_temp_table_task state:running start_date:2025-04-14 04:16:59.453314+00:00
[2025-04-14T04:16:59.533+0000] {logging_mixin.py:190} INFO - Dag name:data_ingestion_gcs_dag and current dag run status:running
[2025-04-14T04:16:59.533+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-04-14T04:16:59.534+0000] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2025-04-14T04:16:59.534+0000] {connection.py:277} WARNING - Connection schemes (type: google_cloud_platform) shall not contain '_' according to RFC3986.
[2025-04-14T04:16:59.535+0000] {base.py:84} INFO - Retrieving connection 'google_cloud_default'
[2025-04-14T04:16:59.562+0000] {bigquery.py:2665} INFO - Executing: {'query': {'query': "\n          CREATE OR REPLACE TABLE `nyc-projects-455321.building_permits.temp_table`\n          AS\n          SELECT\n            borough,\n            job_type,\n            block,\n            lot,\n            PARSE_TIMESTAMP('%m/%d/%Y', issuance_date) AS issuance_date,\n            CAST(permit_si_no AS INT64) AS permit_si_no,\n            CONCAT(gis_latitude,',',gis_longitude) AS location_coordinates,\n            gis_nta_name\n          FROM `nyc-projects-455321.building_permits.external_table`;\n        ", 'useLegacySql': False}}'
[2025-04-14T04:16:59.563+0000] {bigquery.py:1241} INFO - Inserting job ***_data_ingestion_gcs_dag_create_temp_table_task_2025_04_13T00_00_00_00_00_dd2b6ce055f4d2aa7a7cfdffc89a0b9c
[2025-04-14T04:17:02.023+0000] {bigquery.py:2636} INFO - Job ***_data_ingestion_gcs_dag_create_temp_table_task_2025_04_13T00_00_00_00_00_dd2b6ce055f4d2aa7a7cfdffc89a0b9c is completed. Checking the job status
[2025-04-14T04:17:02.067+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-04-14T04:17:02.068+0000] {taskinstance.py:353} INFO - Marking task as SUCCESS. dag_id=data_ingestion_gcs_dag, task_id=create_temp_table_task, run_id=scheduled__2025-04-13T00:00:00+00:00, execution_date=20250413T000000, start_date=20250414T041659, end_date=20250414T041702
[2025-04-14T04:17:02.093+0000] {logging_mixin.py:190} INFO - Task instance in success state
[2025-04-14T04:17:02.093+0000] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2025-04-14T04:17:02.093+0000] {logging_mixin.py:190} INFO - Dag name:data_ingestion_gcs_dag queued_at:2025-04-14 04:16:38.268960+00:00
[2025-04-14T04:17:02.094+0000] {logging_mixin.py:190} INFO - Task hostname:d054e5caec0f operator:BigQueryInsertJobOperator
[2025-04-14T04:17:02.108+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2025-04-14T04:17:02.138+0000] {taskinstance.py:3901} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2025-04-14T04:17:02.141+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-04-14T05:28:22.325+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-04-14T05:28:22.336+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: data_ingestion_gcs_dag.create_temp_table_task scheduled__2025-04-13T00:00:00+00:00 [queued]>
[2025-04-14T05:28:22.341+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: data_ingestion_gcs_dag.create_temp_table_task scheduled__2025-04-13T00:00:00+00:00 [queued]>
[2025-04-14T05:28:22.341+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 2
[2025-04-14T05:28:22.347+0000] {taskinstance.py:2890} INFO - Executing <Task(BigQueryInsertJobOperator): create_temp_table_task> on 2025-04-13 00:00:00+00:00
[2025-04-14T05:28:22.351+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=133) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2025-04-14T05:28:22.352+0000] {standard_task_runner.py:72} INFO - Started process 135 to run task
[2025-04-14T05:28:22.353+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'data_ingestion_gcs_dag', 'create_temp_table_task', 'scheduled__2025-04-13T00:00:00+00:00', '--job-id', '8', '--raw', '--subdir', 'DAGS_FOLDER/data_ingestion_gcs_dag.py', '--cfg-path', '/tmp/tmp79muzdrn']
[2025-04-14T05:28:22.353+0000] {standard_task_runner.py:105} INFO - Job 8: Subtask create_temp_table_task
[2025-04-14T05:28:22.384+0000] {task_command.py:467} INFO - Running <TaskInstance: data_ingestion_gcs_dag.create_temp_table_task scheduled__2025-04-13T00:00:00+00:00 [running]> on host 26629b401d8b
[2025-04-14T05:28:22.422+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='data_ingestion_gcs_dag' AIRFLOW_CTX_TASK_ID='create_temp_table_task' AIRFLOW_CTX_EXECUTION_DATE='2025-04-13T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-04-13T00:00:00+00:00'
[2025-04-14T05:28:22.423+0000] {logging_mixin.py:190} INFO - Task instance is in running state
[2025-04-14T05:28:22.424+0000] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2025-04-14T05:28:22.424+0000] {logging_mixin.py:190} INFO - Current task name:create_temp_table_task state:running start_date:2025-04-14 05:28:22.336825+00:00
[2025-04-14T05:28:22.424+0000] {logging_mixin.py:190} INFO - Dag name:data_ingestion_gcs_dag and current dag run status:running
[2025-04-14T05:28:22.424+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-04-14T05:28:22.425+0000] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2025-04-14T05:28:22.425+0000] {connection.py:277} WARNING - Connection schemes (type: google_cloud_platform) shall not contain '_' according to RFC3986.
[2025-04-14T05:28:22.426+0000] {base.py:84} INFO - Retrieving connection 'google_cloud_default'
[2025-04-14T05:28:22.458+0000] {bigquery.py:2665} INFO - Executing: {'query': {'query': "\n          CREATE OR REPLACE TABLE `nyc-projects-455321.building_permits.temp_table`\n          AS\n          SELECT\n            borough,\n            job_type,\n            block,\n            lot,\n            PARSE_TIMESTAMP('%m/%d/%Y', issuance_date) AS issuance_date,\n            CAST(permit_si_no AS INT64) AS permit_si_no,\n            CONCAT(gis_latitude,',',gis_longitude) AS location_coordinates,\n            gis_nta_name\n          FROM `nyc-projects-455321.building_permits.external_table`;\n        ", 'useLegacySql': False}}'
[2025-04-14T05:28:22.459+0000] {bigquery.py:1241} INFO - Inserting job ***_data_ingestion_gcs_dag_create_temp_table_task_2025_04_13T00_00_00_00_00_fd83213e100f722cecdd66f92a7fa880
[2025-04-14T05:28:25.278+0000] {bigquery.py:2636} INFO - Job ***_data_ingestion_gcs_dag_create_temp_table_task_2025_04_13T00_00_00_00_00_fd83213e100f722cecdd66f92a7fa880 is completed. Checking the job status
[2025-04-14T05:28:25.322+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-04-14T05:28:25.324+0000] {taskinstance.py:353} INFO - Marking task as SUCCESS. dag_id=data_ingestion_gcs_dag, task_id=create_temp_table_task, run_id=scheduled__2025-04-13T00:00:00+00:00, execution_date=20250413T000000, start_date=20250414T052822, end_date=20250414T052825
[2025-04-14T05:28:25.361+0000] {logging_mixin.py:190} INFO - Task instance in success state
[2025-04-14T05:28:25.362+0000] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2025-04-14T05:28:25.362+0000] {logging_mixin.py:190} INFO - Dag name:data_ingestion_gcs_dag queued_at:2025-04-14 05:27:28.560179+00:00
[2025-04-14T05:28:25.363+0000] {logging_mixin.py:190} INFO - Task hostname:26629b401d8b operator:BigQueryInsertJobOperator
[2025-04-14T05:28:25.404+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2025-04-14T05:28:25.412+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-04-14T05:49:28.900+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-04-14T05:49:28.910+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: data_ingestion_gcs_dag.create_temp_table_task scheduled__2025-04-13T00:00:00+00:00 [queued]>
[2025-04-14T05:49:28.916+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: data_ingestion_gcs_dag.create_temp_table_task scheduled__2025-04-13T00:00:00+00:00 [queued]>
[2025-04-14T05:49:28.916+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 2
[2025-04-14T05:49:28.922+0000] {taskinstance.py:2890} INFO - Executing <Task(BigQueryInsertJobOperator): create_temp_table_task> on 2025-04-13 00:00:00+00:00
[2025-04-14T05:49:28.926+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=358) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2025-04-14T05:49:28.927+0000] {standard_task_runner.py:72} INFO - Started process 360 to run task
[2025-04-14T05:49:28.928+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'data_ingestion_gcs_dag', 'create_temp_table_task', 'scheduled__2025-04-13T00:00:00+00:00', '--job-id', '8', '--raw', '--subdir', 'DAGS_FOLDER/data_ingestion_gcs_dag.py', '--cfg-path', '/tmp/tmpb4pbb6mc']
[2025-04-14T05:49:28.928+0000] {standard_task_runner.py:105} INFO - Job 8: Subtask create_temp_table_task
[2025-04-14T05:49:28.954+0000] {task_command.py:467} INFO - Running <TaskInstance: data_ingestion_gcs_dag.create_temp_table_task scheduled__2025-04-13T00:00:00+00:00 [running]> on host 51652b031d9f
[2025-04-14T05:49:28.993+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='data_ingestion_gcs_dag' AIRFLOW_CTX_TASK_ID='create_temp_table_task' AIRFLOW_CTX_EXECUTION_DATE='2025-04-13T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-04-13T00:00:00+00:00'
[2025-04-14T05:49:28.994+0000] {logging_mixin.py:190} INFO - Task instance is in running state
[2025-04-14T05:49:28.994+0000] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2025-04-14T05:49:28.995+0000] {logging_mixin.py:190} INFO - Current task name:create_temp_table_task state:running start_date:2025-04-14 05:49:28.911118+00:00
[2025-04-14T05:49:28.995+0000] {logging_mixin.py:190} INFO - Dag name:data_ingestion_gcs_dag and current dag run status:running
[2025-04-14T05:49:28.995+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-04-14T05:49:28.997+0000] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2025-04-14T05:49:28.997+0000] {connection.py:277} WARNING - Connection schemes (type: google_cloud_platform) shall not contain '_' according to RFC3986.
[2025-04-14T05:49:28.998+0000] {base.py:84} INFO - Retrieving connection 'google_cloud_default'
[2025-04-14T05:49:29.027+0000] {bigquery.py:2665} INFO - Executing: {'query': {'query': "\n          CREATE OR REPLACE TABLE `nyc-projects-455321.building_permits.temp_table`\n          AS\n          SELECT\n            borough,\n            job_type,\n            block,\n            lot,\n            PARSE_TIMESTAMP('%m/%d/%Y', issuance_date) AS issuance_date,\n            CAST(permit_si_no AS INT64) AS permit_si_no,\n            CONCAT(gis_latitude,',',gis_longitude) AS location_coordinates,\n            gis_nta_name\n          FROM `nyc-projects-455321.building_permits.external_table`;\n        ", 'useLegacySql': False}}'
[2025-04-14T05:49:29.027+0000] {bigquery.py:1241} INFO - Inserting job ***_data_ingestion_gcs_dag_create_temp_table_task_2025_04_13T00_00_00_00_00_af814d9755bbd6d19f454f58cbe8f52c
[2025-04-14T05:49:33.281+0000] {bigquery.py:2636} INFO - Job ***_data_ingestion_gcs_dag_create_temp_table_task_2025_04_13T00_00_00_00_00_af814d9755bbd6d19f454f58cbe8f52c is completed. Checking the job status
[2025-04-14T05:49:33.327+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-04-14T05:49:33.328+0000] {taskinstance.py:353} INFO - Marking task as SUCCESS. dag_id=data_ingestion_gcs_dag, task_id=create_temp_table_task, run_id=scheduled__2025-04-13T00:00:00+00:00, execution_date=20250413T000000, start_date=20250414T054928, end_date=20250414T054933
[2025-04-14T05:49:33.370+0000] {logging_mixin.py:190} INFO - Task instance in success state
[2025-04-14T05:49:33.371+0000] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2025-04-14T05:49:33.373+0000] {logging_mixin.py:190} INFO - Dag name:data_ingestion_gcs_dag queued_at:2025-04-14 05:31:46.685490+00:00
[2025-04-14T05:49:33.373+0000] {logging_mixin.py:190} INFO - Task hostname:51652b031d9f operator:BigQueryInsertJobOperator
[2025-04-14T05:49:33.417+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2025-04-14T05:49:33.458+0000] {taskinstance.py:3901} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2025-04-14T05:49:33.459+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
